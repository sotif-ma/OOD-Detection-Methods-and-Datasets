{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Baseline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "environment": {
      "name": "tf2-gpu.2-4.mnightly-2021-01-20-debian-10-test",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:mnightly-2021-01-20-debian-10-test"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayakpaul/Generalized-ODIN-TF/blob/main/Baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFwWgoioRXy9"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCQ1S81KIbaN"
      },
      "source": [
        "# Grab the initial model weights\n",
        "!wget -q https://github.com/sayakpaul/Generalized-ODIN-TF/releases/download/v1.0.0/models.tar.gz\n",
        "!tar xf models.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMOPdB2xSMzv"
      },
      "source": [
        "!git clone https://github.com/sayakpaul/Generalized-ODIN-TF"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtwAr7vgIpS5"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"Generalized-ODIN-TF\")\n",
        "\n",
        "from scripts import resnet20\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWpj-WTpRZvS"
      },
      "source": [
        "## Load CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6mPJL_5JDsO",
        "outputId": "500c0fc7-dd46-45ae-9245-c2ae4ef6111a"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "print(f\"Total training examples: {len(x_train)}\")\n",
        "print(f\"Total test examples: {len(x_test)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total training examples: 50000\n",
            "Total test examples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMIkjg6_RbYc"
      },
      "source": [
        "## Define constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A1ZdFW6J4_R"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 200\n",
        "START_LR = 0.1\n",
        "AUTO = tf.data.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtQQLiGNRdvx"
      },
      "source": [
        "## Prepare data loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqNtOj8yKO_X"
      },
      "source": [
        "# Augmentation pipeline\n",
        "simple_aug = tf.keras.Sequential(\n",
        "    [\n",
        "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
        "        layers.experimental.preprocessing.RandomRotation(factor=0.02),\n",
        "        layers.experimental.preprocessing.RandomZoom(\n",
        "            height_factor=0.2, width_factor=0.2\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Now, map the augmentation pipeline to our training dataset\n",
        "train_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .shuffle(BATCH_SIZE * 100)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .map(lambda x, y: (simple_aug(x), y), num_parallel_calls=AUTO)\n",
        "    .prefetch(AUTO)\n",
        ")\n",
        "\n",
        "# Test dataset\n",
        "test_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(AUTO)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXUFugU3Riph"
      },
      "source": [
        "## Define LR schedule, optimizer, and loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a6VIseo7GFP"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    if epoch < int(EPOCHS * 0.25) - 1:\n",
        "        return START_LR\n",
        "    elif epoch < int(EPOCHS*0.5) -1:\n",
        "        return float(START_LR * 0.1)\n",
        "    elif epoch < int(EPOCHS*0.75) -1:\n",
        "        return float(START_LR * 0.01)\n",
        "    else:\n",
        "        return float(START_LR * 0.001)\n",
        "\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lr_schedule(epoch), verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUhtb9ZiM17B"
      },
      "source": [
        "# Optimizer and loss function.\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=START_LR, momentum=0.9)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkU9afGCRnaC"
      },
      "source": [
        "## Model training with ResNet20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3zhJ1iFPLzp",
        "outputId": "a858aef4-4c17-4b90-cc10-413707d2cbff"
      },
      "source": [
        "rn_model = tf.keras.models.load_model(\"initial_model\")\n",
        "rn_model.compile(loss=loss_fn, optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "history = rn_model.fit(train_ds,\n",
        "    validation_data=test_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[lr_callback])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Epoch 1/200\n",
            "\n",
            "Epoch 00001: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 13s 24ms/step - loss: 2.5417 - accuracy: 0.3458 - val_loss: 2.2469 - val_accuracy: 0.3972\n",
            "Epoch 2/200\n",
            "\n",
            "Epoch 00002: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.6719 - accuracy: 0.5634 - val_loss: 1.6281 - val_accuracy: 0.5601\n",
            "Epoch 3/200\n",
            "\n",
            "Epoch 00003: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.3385 - accuracy: 0.6417 - val_loss: 1.9339 - val_accuracy: 0.5281\n",
            "Epoch 4/200\n",
            "\n",
            "Epoch 00004: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.1944 - accuracy: 0.6821 - val_loss: 1.6186 - val_accuracy: 0.5562\n",
            "Epoch 5/200\n",
            "\n",
            "Epoch 00005: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.1222 - accuracy: 0.7017 - val_loss: 1.6680 - val_accuracy: 0.5813\n",
            "Epoch 6/200\n",
            "\n",
            "Epoch 00006: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.0727 - accuracy: 0.7208 - val_loss: 2.1556 - val_accuracy: 0.5306\n",
            "Epoch 7/200\n",
            "\n",
            "Epoch 00007: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.0375 - accuracy: 0.7357 - val_loss: 1.7324 - val_accuracy: 0.5543\n",
            "Epoch 8/200\n",
            "\n",
            "Epoch 00008: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.0152 - accuracy: 0.7436 - val_loss: 1.1201 - val_accuracy: 0.7067\n",
            "Epoch 9/200\n",
            "\n",
            "Epoch 00009: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 1.0104 - accuracy: 0.7486 - val_loss: 1.5348 - val_accuracy: 0.6013\n",
            "Epoch 10/200\n",
            "\n",
            "Epoch 00010: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9927 - accuracy: 0.7573 - val_loss: 2.1843 - val_accuracy: 0.5091\n",
            "Epoch 11/200\n",
            "\n",
            "Epoch 00011: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9875 - accuracy: 0.7588 - val_loss: 1.7250 - val_accuracy: 0.5691\n",
            "Epoch 12/200\n",
            "\n",
            "Epoch 00012: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9739 - accuracy: 0.7656 - val_loss: 1.5759 - val_accuracy: 0.6138\n",
            "Epoch 13/200\n",
            "\n",
            "Epoch 00013: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9674 - accuracy: 0.7687 - val_loss: 1.7421 - val_accuracy: 0.5875\n",
            "Epoch 14/200\n",
            "\n",
            "Epoch 00014: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9608 - accuracy: 0.7740 - val_loss: 1.6661 - val_accuracy: 0.6155\n",
            "Epoch 15/200\n",
            "\n",
            "Epoch 00015: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9616 - accuracy: 0.7758 - val_loss: 1.3299 - val_accuracy: 0.6701\n",
            "Epoch 16/200\n",
            "\n",
            "Epoch 00016: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9413 - accuracy: 0.7837 - val_loss: 1.3885 - val_accuracy: 0.6630\n",
            "Epoch 17/200\n",
            "\n",
            "Epoch 00017: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9524 - accuracy: 0.7823 - val_loss: 1.6746 - val_accuracy: 0.6159\n",
            "Epoch 18/200\n",
            "\n",
            "Epoch 00018: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9495 - accuracy: 0.7835 - val_loss: 2.4197 - val_accuracy: 0.5118\n",
            "Epoch 19/200\n",
            "\n",
            "Epoch 00019: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9449 - accuracy: 0.7865 - val_loss: 2.2155 - val_accuracy: 0.5142\n",
            "Epoch 20/200\n",
            "\n",
            "Epoch 00020: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9352 - accuracy: 0.7906 - val_loss: 1.2989 - val_accuracy: 0.6986\n",
            "Epoch 21/200\n",
            "\n",
            "Epoch 00021: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9384 - accuracy: 0.7887 - val_loss: 2.0919 - val_accuracy: 0.5349\n",
            "Epoch 22/200\n",
            "\n",
            "Epoch 00022: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9350 - accuracy: 0.7889 - val_loss: 1.3612 - val_accuracy: 0.6501\n",
            "Epoch 23/200\n",
            "\n",
            "Epoch 00023: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9398 - accuracy: 0.7925 - val_loss: 1.9789 - val_accuracy: 0.5679\n",
            "Epoch 24/200\n",
            "\n",
            "Epoch 00024: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9332 - accuracy: 0.7912 - val_loss: 1.1582 - val_accuracy: 0.7239\n",
            "Epoch 25/200\n",
            "\n",
            "Epoch 00025: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9352 - accuracy: 0.7923 - val_loss: 1.4982 - val_accuracy: 0.5941\n",
            "Epoch 26/200\n",
            "\n",
            "Epoch 00026: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9323 - accuracy: 0.7933 - val_loss: 1.4350 - val_accuracy: 0.6555\n",
            "Epoch 27/200\n",
            "\n",
            "Epoch 00027: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9214 - accuracy: 0.8005 - val_loss: 1.4579 - val_accuracy: 0.6360\n",
            "Epoch 28/200\n",
            "\n",
            "Epoch 00028: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9172 - accuracy: 0.8009 - val_loss: 1.3902 - val_accuracy: 0.6784\n",
            "Epoch 29/200\n",
            "\n",
            "Epoch 00029: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9116 - accuracy: 0.8038 - val_loss: 1.3661 - val_accuracy: 0.6537\n",
            "Epoch 30/200\n",
            "\n",
            "Epoch 00030: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9088 - accuracy: 0.8045 - val_loss: 3.3442 - val_accuracy: 0.4581\n",
            "Epoch 31/200\n",
            "\n",
            "Epoch 00031: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9242 - accuracy: 0.8011 - val_loss: 2.0620 - val_accuracy: 0.5317\n",
            "Epoch 32/200\n",
            "\n",
            "Epoch 00032: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9145 - accuracy: 0.8019 - val_loss: 2.5585 - val_accuracy: 0.5001\n",
            "Epoch 33/200\n",
            "\n",
            "Epoch 00033: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9183 - accuracy: 0.8042 - val_loss: 1.2320 - val_accuracy: 0.7161\n",
            "Epoch 34/200\n",
            "\n",
            "Epoch 00034: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9094 - accuracy: 0.8078 - val_loss: 1.3994 - val_accuracy: 0.6923\n",
            "Epoch 35/200\n",
            "\n",
            "Epoch 00035: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9056 - accuracy: 0.8091 - val_loss: 1.0889 - val_accuracy: 0.7548\n",
            "Epoch 36/200\n",
            "\n",
            "Epoch 00036: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9145 - accuracy: 0.8057 - val_loss: 1.3512 - val_accuracy: 0.6813\n",
            "Epoch 37/200\n",
            "\n",
            "Epoch 00037: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9185 - accuracy: 0.8051 - val_loss: 1.3779 - val_accuracy: 0.6688\n",
            "Epoch 38/200\n",
            "\n",
            "Epoch 00038: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9210 - accuracy: 0.8069 - val_loss: 2.4679 - val_accuracy: 0.5107\n",
            "Epoch 39/200\n",
            "\n",
            "Epoch 00039: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9157 - accuracy: 0.8053 - val_loss: 1.8221 - val_accuracy: 0.5977\n",
            "Epoch 40/200\n",
            "\n",
            "Epoch 00040: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9143 - accuracy: 0.8084 - val_loss: 1.2441 - val_accuracy: 0.7302\n",
            "Epoch 41/200\n",
            "\n",
            "Epoch 00041: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9090 - accuracy: 0.8083 - val_loss: 1.5200 - val_accuracy: 0.6177\n",
            "Epoch 42/200\n",
            "\n",
            "Epoch 00042: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9075 - accuracy: 0.8097 - val_loss: 1.9437 - val_accuracy: 0.5758\n",
            "Epoch 43/200\n",
            "\n",
            "Epoch 00043: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.8950 - accuracy: 0.8127 - val_loss: 1.5037 - val_accuracy: 0.6634\n",
            "Epoch 44/200\n",
            "\n",
            "Epoch 00044: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9048 - accuracy: 0.8112 - val_loss: 1.2726 - val_accuracy: 0.7033\n",
            "Epoch 45/200\n",
            "\n",
            "Epoch 00045: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9043 - accuracy: 0.8109 - val_loss: 1.3111 - val_accuracy: 0.6841\n",
            "Epoch 46/200\n",
            "\n",
            "Epoch 00046: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9040 - accuracy: 0.8077 - val_loss: 1.2656 - val_accuracy: 0.7219\n",
            "Epoch 47/200\n",
            "\n",
            "Epoch 00047: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9250 - accuracy: 0.8056 - val_loss: 1.1343 - val_accuracy: 0.7347\n",
            "Epoch 48/200\n",
            "\n",
            "Epoch 00048: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9180 - accuracy: 0.8067 - val_loss: 1.3446 - val_accuracy: 0.6895\n",
            "Epoch 49/200\n",
            "\n",
            "Epoch 00049: LearningRateScheduler reducing learning rate to 0.1.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.9185 - accuracy: 0.8079 - val_loss: 1.7230 - val_accuracy: 0.6400\n",
            "Epoch 50/200\n",
            "\n",
            "Epoch 00050: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.8460 - accuracy: 0.8322 - val_loss: 0.8070 - val_accuracy: 0.8365\n",
            "Epoch 51/200\n",
            "\n",
            "Epoch 00051: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.6727 - accuracy: 0.8832 - val_loss: 0.7510 - val_accuracy: 0.8484\n",
            "Epoch 52/200\n",
            "\n",
            "Epoch 00052: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.6220 - accuracy: 0.8926 - val_loss: 0.7029 - val_accuracy: 0.8599\n",
            "Epoch 53/200\n",
            "\n",
            "Epoch 00053: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5877 - accuracy: 0.8974 - val_loss: 0.6757 - val_accuracy: 0.8655\n",
            "Epoch 54/200\n",
            "\n",
            "Epoch 00054: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5526 - accuracy: 0.9025 - val_loss: 0.6795 - val_accuracy: 0.8578\n",
            "Epoch 55/200\n",
            "\n",
            "Epoch 00055: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.5244 - accuracy: 0.9061 - val_loss: 0.6283 - val_accuracy: 0.8717\n",
            "Epoch 56/200\n",
            "\n",
            "Epoch 00056: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4970 - accuracy: 0.9103 - val_loss: 0.6253 - val_accuracy: 0.8667\n",
            "Epoch 57/200\n",
            "\n",
            "Epoch 00057: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4790 - accuracy: 0.9124 - val_loss: 0.6078 - val_accuracy: 0.8679\n",
            "Epoch 58/200\n",
            "\n",
            "Epoch 00058: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4649 - accuracy: 0.9132 - val_loss: 0.5922 - val_accuracy: 0.8737\n",
            "Epoch 59/200\n",
            "\n",
            "Epoch 00059: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4511 - accuracy: 0.9143 - val_loss: 0.6087 - val_accuracy: 0.8635\n",
            "Epoch 60/200\n",
            "\n",
            "Epoch 00060: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4375 - accuracy: 0.9156 - val_loss: 0.5924 - val_accuracy: 0.8681\n",
            "Epoch 61/200\n",
            "\n",
            "Epoch 00061: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4295 - accuracy: 0.9160 - val_loss: 0.6115 - val_accuracy: 0.8573\n",
            "Epoch 62/200\n",
            "\n",
            "Epoch 00062: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4167 - accuracy: 0.9190 - val_loss: 0.5483 - val_accuracy: 0.8774\n",
            "Epoch 63/200\n",
            "\n",
            "Epoch 00063: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4146 - accuracy: 0.9174 - val_loss: 0.5397 - val_accuracy: 0.8782\n",
            "Epoch 64/200\n",
            "\n",
            "Epoch 00064: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.4129 - accuracy: 0.9172 - val_loss: 0.6891 - val_accuracy: 0.8304\n",
            "Epoch 65/200\n",
            "\n",
            "Epoch 00065: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3943 - accuracy: 0.9203 - val_loss: 0.5968 - val_accuracy: 0.8609\n",
            "Epoch 66/200\n",
            "\n",
            "Epoch 00066: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3964 - accuracy: 0.9187 - val_loss: 0.6264 - val_accuracy: 0.8522\n",
            "Epoch 67/200\n",
            "\n",
            "Epoch 00067: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3949 - accuracy: 0.9176 - val_loss: 0.5620 - val_accuracy: 0.8686\n",
            "Epoch 68/200\n",
            "\n",
            "Epoch 00068: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3827 - accuracy: 0.9232 - val_loss: 0.6935 - val_accuracy: 0.8280\n",
            "Epoch 69/200\n",
            "\n",
            "Epoch 00069: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3876 - accuracy: 0.9189 - val_loss: 0.6329 - val_accuracy: 0.8539\n",
            "Epoch 70/200\n",
            "\n",
            "Epoch 00070: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3832 - accuracy: 0.9218 - val_loss: 0.6409 - val_accuracy: 0.8525\n",
            "Epoch 71/200\n",
            "\n",
            "Epoch 00071: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3797 - accuracy: 0.9215 - val_loss: 0.6468 - val_accuracy: 0.8443\n",
            "Epoch 72/200\n",
            "\n",
            "Epoch 00072: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3876 - accuracy: 0.9177 - val_loss: 0.7193 - val_accuracy: 0.8190\n",
            "Epoch 73/200\n",
            "\n",
            "Epoch 00073: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3804 - accuracy: 0.9210 - val_loss: 0.6219 - val_accuracy: 0.8526\n",
            "Epoch 74/200\n",
            "\n",
            "Epoch 00074: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3825 - accuracy: 0.9235 - val_loss: 0.5951 - val_accuracy: 0.8571\n",
            "Epoch 75/200\n",
            "\n",
            "Epoch 00075: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3818 - accuracy: 0.9239 - val_loss: 0.5825 - val_accuracy: 0.8654\n",
            "Epoch 76/200\n",
            "\n",
            "Epoch 00076: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3796 - accuracy: 0.9232 - val_loss: 0.6049 - val_accuracy: 0.8549\n",
            "Epoch 77/200\n",
            "\n",
            "Epoch 00077: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3814 - accuracy: 0.9207 - val_loss: 0.6591 - val_accuracy: 0.8434\n",
            "Epoch 78/200\n",
            "\n",
            "Epoch 00078: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3713 - accuracy: 0.9263 - val_loss: 0.6908 - val_accuracy: 0.8369\n",
            "Epoch 79/200\n",
            "\n",
            "Epoch 00079: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3816 - accuracy: 0.9218 - val_loss: 0.6782 - val_accuracy: 0.8403\n",
            "Epoch 80/200\n",
            "\n",
            "Epoch 00080: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3766 - accuracy: 0.9256 - val_loss: 0.7142 - val_accuracy: 0.8316\n",
            "Epoch 81/200\n",
            "\n",
            "Epoch 00081: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3724 - accuracy: 0.9256 - val_loss: 0.7535 - val_accuracy: 0.8207\n",
            "Epoch 82/200\n",
            "\n",
            "Epoch 00082: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3767 - accuracy: 0.9241 - val_loss: 0.6764 - val_accuracy: 0.8433\n",
            "Epoch 83/200\n",
            "\n",
            "Epoch 00083: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3765 - accuracy: 0.9248 - val_loss: 0.6370 - val_accuracy: 0.8497\n",
            "Epoch 84/200\n",
            "\n",
            "Epoch 00084: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3711 - accuracy: 0.9257 - val_loss: 0.5929 - val_accuracy: 0.8622\n",
            "Epoch 85/200\n",
            "\n",
            "Epoch 00085: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3721 - accuracy: 0.9269 - val_loss: 0.7830 - val_accuracy: 0.8210\n",
            "Epoch 86/200\n",
            "\n",
            "Epoch 00086: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3679 - accuracy: 0.9276 - val_loss: 0.5934 - val_accuracy: 0.8619\n",
            "Epoch 87/200\n",
            "\n",
            "Epoch 00087: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3727 - accuracy: 0.9263 - val_loss: 0.5635 - val_accuracy: 0.8678\n",
            "Epoch 88/200\n",
            "\n",
            "Epoch 00088: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3766 - accuracy: 0.9269 - val_loss: 0.6558 - val_accuracy: 0.8387\n",
            "Epoch 89/200\n",
            "\n",
            "Epoch 00089: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3742 - accuracy: 0.9280 - val_loss: 0.5834 - val_accuracy: 0.8693\n",
            "Epoch 90/200\n",
            "\n",
            "Epoch 00090: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3713 - accuracy: 0.9271 - val_loss: 0.6529 - val_accuracy: 0.8503\n",
            "Epoch 91/200\n",
            "\n",
            "Epoch 00091: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3730 - accuracy: 0.9273 - val_loss: 0.6813 - val_accuracy: 0.8470\n",
            "Epoch 92/200\n",
            "\n",
            "Epoch 00092: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3726 - accuracy: 0.9294 - val_loss: 0.6100 - val_accuracy: 0.8617\n",
            "Epoch 93/200\n",
            "\n",
            "Epoch 00093: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3751 - accuracy: 0.9259 - val_loss: 0.5983 - val_accuracy: 0.8717\n",
            "Epoch 94/200\n",
            "\n",
            "Epoch 00094: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3689 - accuracy: 0.9292 - val_loss: 0.6930 - val_accuracy: 0.8393\n",
            "Epoch 95/200\n",
            "\n",
            "Epoch 00095: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3726 - accuracy: 0.9290 - val_loss: 0.7494 - val_accuracy: 0.8310\n",
            "Epoch 96/200\n",
            "\n",
            "Epoch 00096: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3680 - accuracy: 0.9308 - val_loss: 0.7756 - val_accuracy: 0.8219\n",
            "Epoch 97/200\n",
            "\n",
            "Epoch 00097: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3616 - accuracy: 0.9350 - val_loss: 0.6581 - val_accuracy: 0.8564\n",
            "Epoch 98/200\n",
            "\n",
            "Epoch 00098: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3742 - accuracy: 0.9271 - val_loss: 0.7868 - val_accuracy: 0.8244\n",
            "Epoch 99/200\n",
            "\n",
            "Epoch 00099: LearningRateScheduler reducing learning rate to 0.010000000000000002.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3671 - accuracy: 0.9317 - val_loss: 0.8255 - val_accuracy: 0.8214\n",
            "Epoch 100/200\n",
            "\n",
            "Epoch 00100: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.3465 - accuracy: 0.9393 - val_loss: 0.4757 - val_accuracy: 0.9004\n",
            "Epoch 101/200\n",
            "\n",
            "Epoch 00101: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2949 - accuracy: 0.9607 - val_loss: 0.4523 - val_accuracy: 0.9051\n",
            "Epoch 102/200\n",
            "\n",
            "Epoch 00102: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2861 - accuracy: 0.9617 - val_loss: 0.4608 - val_accuracy: 0.9042\n",
            "Epoch 103/200\n",
            "\n",
            "Epoch 00103: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2760 - accuracy: 0.9657 - val_loss: 0.4574 - val_accuracy: 0.9059\n",
            "Epoch 104/200\n",
            "\n",
            "Epoch 00104: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2657 - accuracy: 0.9687 - val_loss: 0.4459 - val_accuracy: 0.9086\n",
            "Epoch 105/200\n",
            "\n",
            "Epoch 00105: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2647 - accuracy: 0.9687 - val_loss: 0.4481 - val_accuracy: 0.9077\n",
            "Epoch 106/200\n",
            "\n",
            "Epoch 00106: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2590 - accuracy: 0.9718 - val_loss: 0.4443 - val_accuracy: 0.9093\n",
            "Epoch 107/200\n",
            "\n",
            "Epoch 00107: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2541 - accuracy: 0.9717 - val_loss: 0.4511 - val_accuracy: 0.9040\n",
            "Epoch 108/200\n",
            "\n",
            "Epoch 00108: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2533 - accuracy: 0.9721 - val_loss: 0.4495 - val_accuracy: 0.9045\n",
            "Epoch 109/200\n",
            "\n",
            "Epoch 00109: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2439 - accuracy: 0.9738 - val_loss: 0.4460 - val_accuracy: 0.9067\n",
            "Epoch 110/200\n",
            "\n",
            "Epoch 00110: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2441 - accuracy: 0.9740 - val_loss: 0.4362 - val_accuracy: 0.9100\n",
            "Epoch 111/200\n",
            "\n",
            "Epoch 00111: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2384 - accuracy: 0.9756 - val_loss: 0.4499 - val_accuracy: 0.9054\n",
            "Epoch 112/200\n",
            "\n",
            "Epoch 00112: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2385 - accuracy: 0.9745 - val_loss: 0.4259 - val_accuracy: 0.9125\n",
            "Epoch 113/200\n",
            "\n",
            "Epoch 00113: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2333 - accuracy: 0.9783 - val_loss: 0.4408 - val_accuracy: 0.9078\n",
            "Epoch 114/200\n",
            "\n",
            "Epoch 00114: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2295 - accuracy: 0.9777 - val_loss: 0.4423 - val_accuracy: 0.9104\n",
            "Epoch 115/200\n",
            "\n",
            "Epoch 00115: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2281 - accuracy: 0.9775 - val_loss: 0.4539 - val_accuracy: 0.9052\n",
            "Epoch 116/200\n",
            "\n",
            "Epoch 00116: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2244 - accuracy: 0.9786 - val_loss: 0.4430 - val_accuracy: 0.9081\n",
            "Epoch 117/200\n",
            "\n",
            "Epoch 00117: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2218 - accuracy: 0.9787 - val_loss: 0.4360 - val_accuracy: 0.9107\n",
            "Epoch 118/200\n",
            "\n",
            "Epoch 00118: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2207 - accuracy: 0.9796 - val_loss: 0.4370 - val_accuracy: 0.9088\n",
            "Epoch 119/200\n",
            "\n",
            "Epoch 00119: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2175 - accuracy: 0.9790 - val_loss: 0.4308 - val_accuracy: 0.9105\n",
            "Epoch 120/200\n",
            "\n",
            "Epoch 00120: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2141 - accuracy: 0.9801 - val_loss: 0.4333 - val_accuracy: 0.9126\n",
            "Epoch 121/200\n",
            "\n",
            "Epoch 00121: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2125 - accuracy: 0.9807 - val_loss: 0.4286 - val_accuracy: 0.9119\n",
            "Epoch 122/200\n",
            "\n",
            "Epoch 00122: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2096 - accuracy: 0.9814 - val_loss: 0.4278 - val_accuracy: 0.9119\n",
            "Epoch 123/200\n",
            "\n",
            "Epoch 00123: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2090 - accuracy: 0.9807 - val_loss: 0.4308 - val_accuracy: 0.9101\n",
            "Epoch 124/200\n",
            "\n",
            "Epoch 00124: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2078 - accuracy: 0.9812 - val_loss: 0.4345 - val_accuracy: 0.9107\n",
            "Epoch 125/200\n",
            "\n",
            "Epoch 00125: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2034 - accuracy: 0.9823 - val_loss: 0.4335 - val_accuracy: 0.9092\n",
            "Epoch 126/200\n",
            "\n",
            "Epoch 00126: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.2055 - accuracy: 0.9813 - val_loss: 0.4254 - val_accuracy: 0.9105\n",
            "Epoch 127/200\n",
            "\n",
            "Epoch 00127: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1985 - accuracy: 0.9839 - val_loss: 0.4292 - val_accuracy: 0.9130\n",
            "Epoch 128/200\n",
            "\n",
            "Epoch 00128: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1961 - accuracy: 0.9847 - val_loss: 0.4474 - val_accuracy: 0.9061\n",
            "Epoch 129/200\n",
            "\n",
            "Epoch 00129: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1974 - accuracy: 0.9828 - val_loss: 0.4351 - val_accuracy: 0.9101\n",
            "Epoch 130/200\n",
            "\n",
            "Epoch 00130: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1948 - accuracy: 0.9843 - val_loss: 0.4290 - val_accuracy: 0.9103\n",
            "Epoch 131/200\n",
            "\n",
            "Epoch 00131: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1923 - accuracy: 0.9849 - val_loss: 0.4319 - val_accuracy: 0.9092\n",
            "Epoch 132/200\n",
            "\n",
            "Epoch 00132: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1938 - accuracy: 0.9823 - val_loss: 0.4226 - val_accuracy: 0.9130\n",
            "Epoch 133/200\n",
            "\n",
            "Epoch 00133: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1927 - accuracy: 0.9833 - val_loss: 0.4301 - val_accuracy: 0.9114\n",
            "Epoch 134/200\n",
            "\n",
            "Epoch 00134: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1881 - accuracy: 0.9845 - val_loss: 0.4299 - val_accuracy: 0.9097\n",
            "Epoch 135/200\n",
            "\n",
            "Epoch 00135: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1874 - accuracy: 0.9842 - val_loss: 0.4322 - val_accuracy: 0.9097\n",
            "Epoch 136/200\n",
            "\n",
            "Epoch 00136: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1864 - accuracy: 0.9847 - val_loss: 0.4324 - val_accuracy: 0.9074\n",
            "Epoch 137/200\n",
            "\n",
            "Epoch 00137: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1814 - accuracy: 0.9858 - val_loss: 0.4311 - val_accuracy: 0.9083\n",
            "Epoch 138/200\n",
            "\n",
            "Epoch 00138: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1815 - accuracy: 0.9847 - val_loss: 0.4129 - val_accuracy: 0.9137\n",
            "Epoch 139/200\n",
            "\n",
            "Epoch 00139: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1841 - accuracy: 0.9841 - val_loss: 0.4348 - val_accuracy: 0.9093\n",
            "Epoch 140/200\n",
            "\n",
            "Epoch 00140: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1825 - accuracy: 0.9837 - val_loss: 0.4369 - val_accuracy: 0.9068\n",
            "Epoch 141/200\n",
            "\n",
            "Epoch 00141: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1773 - accuracy: 0.9858 - val_loss: 0.4137 - val_accuracy: 0.9133\n",
            "Epoch 142/200\n",
            "\n",
            "Epoch 00142: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1758 - accuracy: 0.9861 - val_loss: 0.4160 - val_accuracy: 0.9124\n",
            "Epoch 143/200\n",
            "\n",
            "Epoch 00143: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1745 - accuracy: 0.9858 - val_loss: 0.4186 - val_accuracy: 0.9101\n",
            "Epoch 144/200\n",
            "\n",
            "Epoch 00144: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1751 - accuracy: 0.9852 - val_loss: 0.4201 - val_accuracy: 0.9118\n",
            "Epoch 145/200\n",
            "\n",
            "Epoch 00145: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1690 - accuracy: 0.9880 - val_loss: 0.4208 - val_accuracy: 0.9138\n",
            "Epoch 146/200\n",
            "\n",
            "Epoch 00146: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1698 - accuracy: 0.9866 - val_loss: 0.4279 - val_accuracy: 0.9112\n",
            "Epoch 147/200\n",
            "\n",
            "Epoch 00147: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1676 - accuracy: 0.9866 - val_loss: 0.4224 - val_accuracy: 0.9117\n",
            "Epoch 148/200\n",
            "\n",
            "Epoch 00148: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1680 - accuracy: 0.9865 - val_loss: 0.4263 - val_accuracy: 0.9106\n",
            "Epoch 149/200\n",
            "\n",
            "Epoch 00149: LearningRateScheduler reducing learning rate to 0.001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1671 - accuracy: 0.9872 - val_loss: 0.4134 - val_accuracy: 0.9132\n",
            "Epoch 150/200\n",
            "\n",
            "Epoch 00150: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1648 - accuracy: 0.9873 - val_loss: 0.4149 - val_accuracy: 0.9134\n",
            "Epoch 151/200\n",
            "\n",
            "Epoch 00151: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1622 - accuracy: 0.9894 - val_loss: 0.4136 - val_accuracy: 0.9132\n",
            "Epoch 152/200\n",
            "\n",
            "Epoch 00152: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1603 - accuracy: 0.9899 - val_loss: 0.4106 - val_accuracy: 0.9148\n",
            "Epoch 153/200\n",
            "\n",
            "Epoch 00153: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1603 - accuracy: 0.9891 - val_loss: 0.4119 - val_accuracy: 0.9137\n",
            "Epoch 154/200\n",
            "\n",
            "Epoch 00154: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1582 - accuracy: 0.9903 - val_loss: 0.4121 - val_accuracy: 0.9126\n",
            "Epoch 155/200\n",
            "\n",
            "Epoch 00155: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1594 - accuracy: 0.9893 - val_loss: 0.4125 - val_accuracy: 0.9133\n",
            "Epoch 156/200\n",
            "\n",
            "Epoch 00156: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1579 - accuracy: 0.9900 - val_loss: 0.4102 - val_accuracy: 0.9137\n",
            "Epoch 157/200\n",
            "\n",
            "Epoch 00157: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1599 - accuracy: 0.9896 - val_loss: 0.4122 - val_accuracy: 0.9142\n",
            "Epoch 158/200\n",
            "\n",
            "Epoch 00158: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1575 - accuracy: 0.9903 - val_loss: 0.4092 - val_accuracy: 0.9154\n",
            "Epoch 159/200\n",
            "\n",
            "Epoch 00159: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1565 - accuracy: 0.9905 - val_loss: 0.4131 - val_accuracy: 0.9143\n",
            "Epoch 160/200\n",
            "\n",
            "Epoch 00160: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1569 - accuracy: 0.9900 - val_loss: 0.4093 - val_accuracy: 0.9145\n",
            "Epoch 161/200\n",
            "\n",
            "Epoch 00161: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1588 - accuracy: 0.9894 - val_loss: 0.4111 - val_accuracy: 0.9142\n",
            "Epoch 162/200\n",
            "\n",
            "Epoch 00162: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1565 - accuracy: 0.9907 - val_loss: 0.4111 - val_accuracy: 0.9155\n",
            "Epoch 163/200\n",
            "\n",
            "Epoch 00163: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1574 - accuracy: 0.9904 - val_loss: 0.4101 - val_accuracy: 0.9149\n",
            "Epoch 164/200\n",
            "\n",
            "Epoch 00164: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1561 - accuracy: 0.9909 - val_loss: 0.4138 - val_accuracy: 0.9129\n",
            "Epoch 165/200\n",
            "\n",
            "Epoch 00165: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1551 - accuracy: 0.9905 - val_loss: 0.4123 - val_accuracy: 0.9141\n",
            "Epoch 166/200\n",
            "\n",
            "Epoch 00166: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1562 - accuracy: 0.9906 - val_loss: 0.4108 - val_accuracy: 0.9135\n",
            "Epoch 167/200\n",
            "\n",
            "Epoch 00167: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1554 - accuracy: 0.9904 - val_loss: 0.4131 - val_accuracy: 0.9135\n",
            "Epoch 168/200\n",
            "\n",
            "Epoch 00168: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1547 - accuracy: 0.9900 - val_loss: 0.4121 - val_accuracy: 0.9145\n",
            "Epoch 169/200\n",
            "\n",
            "Epoch 00169: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1542 - accuracy: 0.9912 - val_loss: 0.4116 - val_accuracy: 0.9141\n",
            "Epoch 170/200\n",
            "\n",
            "Epoch 00170: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1549 - accuracy: 0.9908 - val_loss: 0.4116 - val_accuracy: 0.9138\n",
            "Epoch 171/200\n",
            "\n",
            "Epoch 00171: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1547 - accuracy: 0.9908 - val_loss: 0.4123 - val_accuracy: 0.9142\n",
            "Epoch 172/200\n",
            "\n",
            "Epoch 00172: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1527 - accuracy: 0.9920 - val_loss: 0.4115 - val_accuracy: 0.9128\n",
            "Epoch 173/200\n",
            "\n",
            "Epoch 00173: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1534 - accuracy: 0.9911 - val_loss: 0.4102 - val_accuracy: 0.9133\n",
            "Epoch 174/200\n",
            "\n",
            "Epoch 00174: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1539 - accuracy: 0.9911 - val_loss: 0.4103 - val_accuracy: 0.9138\n",
            "Epoch 175/200\n",
            "\n",
            "Epoch 00175: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1541 - accuracy: 0.9908 - val_loss: 0.4109 - val_accuracy: 0.9130\n",
            "Epoch 176/200\n",
            "\n",
            "Epoch 00176: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1544 - accuracy: 0.9905 - val_loss: 0.4132 - val_accuracy: 0.9144\n",
            "Epoch 177/200\n",
            "\n",
            "Epoch 00177: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1512 - accuracy: 0.9919 - val_loss: 0.4127 - val_accuracy: 0.9137\n",
            "Epoch 178/200\n",
            "\n",
            "Epoch 00178: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1535 - accuracy: 0.9906 - val_loss: 0.4125 - val_accuracy: 0.9131\n",
            "Epoch 179/200\n",
            "\n",
            "Epoch 00179: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1532 - accuracy: 0.9907 - val_loss: 0.4116 - val_accuracy: 0.9143\n",
            "Epoch 180/200\n",
            "\n",
            "Epoch 00180: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1526 - accuracy: 0.9908 - val_loss: 0.4108 - val_accuracy: 0.9157\n",
            "Epoch 181/200\n",
            "\n",
            "Epoch 00181: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1518 - accuracy: 0.9918 - val_loss: 0.4112 - val_accuracy: 0.9150\n",
            "Epoch 182/200\n",
            "\n",
            "Epoch 00182: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1532 - accuracy: 0.9903 - val_loss: 0.4108 - val_accuracy: 0.9152\n",
            "Epoch 183/200\n",
            "\n",
            "Epoch 00183: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1513 - accuracy: 0.9920 - val_loss: 0.4120 - val_accuracy: 0.9155\n",
            "Epoch 184/200\n",
            "\n",
            "Epoch 00184: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1527 - accuracy: 0.9914 - val_loss: 0.4102 - val_accuracy: 0.9161\n",
            "Epoch 185/200\n",
            "\n",
            "Epoch 00185: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1539 - accuracy: 0.9897 - val_loss: 0.4125 - val_accuracy: 0.9146\n",
            "Epoch 186/200\n",
            "\n",
            "Epoch 00186: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1523 - accuracy: 0.9913 - val_loss: 0.4122 - val_accuracy: 0.9145\n",
            "Epoch 187/200\n",
            "\n",
            "Epoch 00187: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1512 - accuracy: 0.9914 - val_loss: 0.4143 - val_accuracy: 0.9145\n",
            "Epoch 188/200\n",
            "\n",
            "Epoch 00188: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1513 - accuracy: 0.9915 - val_loss: 0.4111 - val_accuracy: 0.9157\n",
            "Epoch 189/200\n",
            "\n",
            "Epoch 00189: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1520 - accuracy: 0.9911 - val_loss: 0.4110 - val_accuracy: 0.9150\n",
            "Epoch 190/200\n",
            "\n",
            "Epoch 00190: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1515 - accuracy: 0.9912 - val_loss: 0.4106 - val_accuracy: 0.9145\n",
            "Epoch 191/200\n",
            "\n",
            "Epoch 00191: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1508 - accuracy: 0.9923 - val_loss: 0.4099 - val_accuracy: 0.9149\n",
            "Epoch 192/200\n",
            "\n",
            "Epoch 00192: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1506 - accuracy: 0.9909 - val_loss: 0.4095 - val_accuracy: 0.9138\n",
            "Epoch 193/200\n",
            "\n",
            "Epoch 00193: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1517 - accuracy: 0.9911 - val_loss: 0.4091 - val_accuracy: 0.9147\n",
            "Epoch 194/200\n",
            "\n",
            "Epoch 00194: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1503 - accuracy: 0.9914 - val_loss: 0.4104 - val_accuracy: 0.9148\n",
            "Epoch 195/200\n",
            "\n",
            "Epoch 00195: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1511 - accuracy: 0.9906 - val_loss: 0.4108 - val_accuracy: 0.9144\n",
            "Epoch 196/200\n",
            "\n",
            "Epoch 00196: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1486 - accuracy: 0.9923 - val_loss: 0.4110 - val_accuracy: 0.9139\n",
            "Epoch 197/200\n",
            "\n",
            "Epoch 00197: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1511 - accuracy: 0.9905 - val_loss: 0.4113 - val_accuracy: 0.9149\n",
            "Epoch 198/200\n",
            "\n",
            "Epoch 00198: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1494 - accuracy: 0.9919 - val_loss: 0.4086 - val_accuracy: 0.9148\n",
            "Epoch 199/200\n",
            "\n",
            "Epoch 00199: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1500 - accuracy: 0.9918 - val_loss: 0.4099 - val_accuracy: 0.9150\n",
            "Epoch 200/200\n",
            "\n",
            "Epoch 00200: LearningRateScheduler reducing learning rate to 0.0001.\n",
            "391/391 [==============================] - 8s 21ms/step - loss: 0.1485 - accuracy: 0.9922 - val_loss: 0.4116 - val_accuracy: 0.9142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "QPWAgdc65Ppc",
        "outputId": "24617f95-2bf1-4e61-a469-ae0589a4441d"
      },
      "source": [
        "plt.plot(history.history[\"loss\"], label=\"train loss\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"test loss\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "rn_model.save(\"rn_baseline_model\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: rn_baseline_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJumO7y9SLuo",
        "outputId": "4cfc3fc6-3f8b-4739-f583-00a608287cde"
      },
      "source": [
        "_, train_acc = rn_model.evaluate(train_ds, verbose=0)\n",
        "_, test_acc = rn_model.evaluate(test_ds, verbose=0)\n",
        "print(\"Train accuracy: {:.2f}%\".format(train_acc * 100))\n",
        "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train accuracy: 99.46%\n",
            "Test accuracy: 91.42%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}