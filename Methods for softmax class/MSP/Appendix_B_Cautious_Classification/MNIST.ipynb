{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# training parameters\n",
    "training_epochs = 30\n",
    "batch_size = 128\n",
    "\n",
    "# architecture parameters\n",
    "n_labels = 10\n",
    "image_pixels = 28 * 28\n",
    "bottleneck = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_and_test(mode=\"c_is_softmax_prob\", seed=100, learning_rate=0.001):\n",
    "    '''\n",
    "    modes: c_is_softmax_prob, c_is_trained_softmax_prob, c_is_cotrained_sigmoid, c_is_auxiliary_sigmoid\n",
    "    '''\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)  # seed set upon graph construction; does not work\n",
    "\n",
    "        x = tf.placeholder(dtype=tf.float32, shape=[None, image_pixels])\n",
    "        y = tf.placeholder(dtype=tf.float32, shape=[None, n_labels])\n",
    "\n",
    "        def gelu(x):\n",
    "            return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "        f = gelu\n",
    "\n",
    "        W = {}\n",
    "        b = {}\n",
    "\n",
    "        with tf.variable_scope(\"classifier\"):\n",
    "            W['1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([image_pixels, 256]), 0))\n",
    "            W['2'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([256, 256]), 0))\n",
    "            W['3'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([256, 256]), 0))\n",
    "            W['logits'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([256, n_labels]), 0))\n",
    "\n",
    "            b['1'] = tf.Variable(tf.zeros([256]))\n",
    "            b['2'] = tf.Variable(tf.zeros([256]))\n",
    "            b['3'] = tf.Variable(tf.zeros([256]))\n",
    "            b['logits'] = tf.Variable(tf.zeros([n_labels]))\n",
    "\n",
    "        with tf.variable_scope(\"confidence_scorer\"):\n",
    "            W['hidden_to_conf1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([256, 512]), 0))\n",
    "            W['logits_to_conf1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_labels, 512]), 0))\n",
    "            W['conf2'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([512, 128]), 0))\n",
    "            W['conf'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([128, 1]), 0))\n",
    "\n",
    "            b['conf1'] = tf.Variable(tf.zeros([512]))\n",
    "            b['conf2'] = tf.Variable(tf.zeros([128]))\n",
    "            b['conf'] = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "        def cautious_fcn(x):\n",
    "            h1 = f(tf.matmul(x, W['1']) + b['1'])\n",
    "            h2 = f(tf.matmul(h1, W['2']) + b['2'])\n",
    "            h3 = f(tf.matmul(h2, W['3']) + b['3'])\n",
    "            logits_out = tf.matmul(h3, W['logits']) + b['logits']\n",
    "\n",
    "            conf1 = f(tf.matmul(logits_out, W['logits_to_conf1']) +\n",
    "                        tf.matmul(h2, W['hidden_to_conf1']) + b['conf1'])\n",
    "            conf2 = f(tf.matmul(conf1, W['conf2']) + b['conf2'])\n",
    "            conf_out = tf.matmul(conf2, W['conf']) + b['conf']\n",
    "\n",
    "            return logits_out, tf.squeeze(conf_out)\n",
    "\n",
    "        logits, confidence_logit = cautious_fcn(x)\n",
    "\n",
    "        right_answer = tf.stop_gradient(tf.to_float(tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))))\n",
    "        compute_error = 100*tf.reduce_mean(1 - right_answer)\n",
    "\n",
    "        classification_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))\n",
    "        if \"softmax\" in mode:\n",
    "            confidence_logit = tf.reduce_max(tf.nn.softmax(logits), reduction_indices=[1])\n",
    "            caution_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(confidence_logit, right_answer))\n",
    "            \n",
    "            # cc_loss is cautious classification loss\n",
    "            if mode == \"c_is_trained_softmax_prob\":\n",
    "                cc_loss = classification_loss + caution_loss\n",
    "            else:\n",
    "                cc_loss = classification_loss\n",
    "        \n",
    "        elif mode == \"c_is_cotrained_sigmoid\":\n",
    "            caution_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(confidence_logit, right_answer))\n",
    "            cc_loss = classification_loss + caution_loss\n",
    "            confidence = tf.sigmoid(confidence_logit)\n",
    "        elif mode == \"c_is_auxiliary_sigmoid\":\n",
    "            caution_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(confidence_logit, right_answer))\n",
    "            cc_loss = classification_loss  # we use caution_loss after training normal classifier\n",
    "        else:\n",
    "            assert False, \"Invalid mode specified\"\n",
    "        \n",
    "        cc_calibration_score = tf.reduce_mean((2 * right_answer - 1) * (2 * tf.sigmoid(confidence_logit) - 1))\n",
    "        cc_model_score = tf.reduce_mean(right_answer * ((2 * right_answer - 1) * (2 * tf.sigmoid(confidence_logit) - 1)+ 1)/2)\n",
    "        \n",
    "        # cautious classification perplexity\n",
    "        cc_calibration_perplexity = tf.exp(caution_loss)\n",
    "        cc_model_perplexity = tf.exp(caution_loss + classification_loss)\n",
    "        \n",
    "        lr = tf.constant(learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cc_loss)\n",
    "\n",
    "    sess = tf.InteractiveSession(graph=graph)\n",
    "    \n",
    "    if \"softmax\" in mode:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    elif mode == \"c_is_cotrained_sigmoid\":\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    elif mode == \"c_is_auxiliary_sigmoid\":\n",
    "        thawed_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"classifier\")\n",
    "        frozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"confidence_scorer\")\n",
    "        sess.run(tf.initialize_variables(set(tf.all_variables()) - set(frozen_vars)))\n",
    "    \n",
    "    err_ema = 90\n",
    "    cc_calibration_perp_ema = 10\n",
    "    cc_model_perp_ema = 10\n",
    "    cc_calibration_score_ema = -1\n",
    "    cc_model_score_ema = -1\n",
    "    num_batches = (mnist.train.num_examples + mnist.validation.num_examples) // batch_size\n",
    "    \n",
    "    for epoch in range(1,training_epochs+1):\n",
    "        if epoch >= 20:\n",
    "            learning_rate *= 0.1\n",
    "        for i in range(num_batches):\n",
    "            if i < mnist.train.num_examples//batch_size: \n",
    "                bx, by = mnist.train.next_batch(batch_size)\n",
    "            else:  # there is no need to hold out the validation set\n",
    "                bx, by = mnist.validation.next_batch(batch_size)\n",
    "                \n",
    "            if mode != \"c_is_auxiliary_sigmoid\":\n",
    "                _, err, cc_model_score_curr, cc_calibration_score_curr,\\\n",
    "                cc_model_perp_curr, cc_calibration_perp_curr = sess.run([\n",
    "                        optimizer, compute_error, cc_model_score, cc_calibration_score,\n",
    "                        cc_model_perplexity, cc_calibration_perplexity],\n",
    "                     feed_dict={x: bx, y: by, lr: learning_rate})\n",
    "                \n",
    "                err_ema = err_ema * 0.95 + 0.05 * err\n",
    "                cc_calibration_perp_ema = cc_calibration_perp_ema * 0.95 + 0.05 * cc_calibration_perp_curr\n",
    "                cc_model_perp_ema = cc_model_perp_ema * 0.95 + 0.05 * cc_model_perp_curr\n",
    "                cc_calibration_score_ema = cc_calibration_score_ema * 0.95 + 0.05 * cc_calibration_score_curr\n",
    "                cc_model_score_ema = cc_model_score_ema * 0.95 + 0.05 * cc_model_score_curr\n",
    "            else:\n",
    "                _, err, l = sess.run([optimizer, compute_error, cc_loss],\n",
    "                                     feed_dict={x: bx, y: by, lr: learning_rate})\n",
    "                err_ema = err_ema * 0.95 + 0.05 * err\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch', epoch, ' | ', 'Current Classification Error (%)', err_ema)\n",
    "            if mode != \"c_is_auxiliary_sigmoid\":\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Calibration Perp', cc_calibration_perp_ema)\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Model Perp', cc_model_perp_ema)\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Calibration Score', cc_calibration_score_ema)\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Model Score', cc_model_score_ema)\n",
    "\n",
    "    if mode == \"c_is_auxiliary_sigmoid\":\n",
    "        # train sigmoid separately from the classifier\n",
    "        phase2_vars = list(set(tf.all_variables()) - set(thawed_vars))\n",
    "        optimizer2 = tf.train.AdamOptimizer(learning_rate=0.001).minimize(caution_loss, var_list=phase2_vars)\n",
    "        sess.run(tf.initialize_variables(set(tf.all_variables()) - set(thawed_vars)))\n",
    "        \n",
    "        for epoch in range(5):\n",
    "            for i in range(num_batches):\n",
    "                if i < mnist.train.num_examples//batch_size: \n",
    "                    bx, by = mnist.train.next_batch(batch_size)\n",
    "                else:  # there is no need to hold out the validation set\n",
    "                    bx, by = mnist.validation.next_batch(batch_size)\n",
    "\n",
    "                sess.run([optimizer2], feed_dict={x: bx, y: by})\n",
    "\n",
    "    err, cc_model_score_test, cc_calibration_score_test,\\\n",
    "    cc_model_perp_test, cc_calibration_perp_test = sess.run([\n",
    "                    compute_error, cc_model_score, cc_calibration_score,\n",
    "                    cc_model_perplexity, cc_calibration_perplexity],\n",
    "                                  feed_dict={x: mnist.test.images, y: mnist.test.labels})\n",
    "\n",
    "    print('Test Classification Error (%)', err)\n",
    "    print('Test Cautious Classification Calibration Perp', cc_calibration_perp_test)\n",
    "    print('Test Cautious Classification Model Perp', cc_model_perp_test)\n",
    "    print('Test Cautious Classification Calibration Score', cc_calibration_score_test)\n",
    "    print('Test Cautious Classification Model Score', cc_model_score_test)\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  |  Current Classification Error (%) 0.238838601712\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.3716098518\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.38537145556\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.459150262912\n",
      "Epoch 10  |  Cautious Classification Model Score 0.728819428557\n",
      "Epoch 20  |  Current Classification Error (%) 4.74314965953e-05\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.36802734231\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.36858499174\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.461961028479\n",
      "Epoch 20  |  Cautious Classification Model Score 0.730980158868\n",
      "Epoch 30  |  Current Classification Error (%) 5.34356365369e-07\n",
      "Epoch 30  |  Cautious Classification Calibration Perp 1.3679960514\n",
      "Epoch 30  |  Cautious Classification Model Perp 1.36843520308\n",
      "Epoch 30  |  Cautious Classification Calibration Score 0.461993655648\n",
      "Epoch 30  |  Cautious Classification Model Score 0.730996612043\n",
      "Test Classification Error (%) 1.53\n",
      "Test Cautious Classification Calibration Perp 1.38784\n",
      "Test Cautious Classification Model Perp 1.49883\n",
      "Test Cautious Classification Calibration Score 0.447779\n",
      "Test Cautious Classification Model Score 0.719284\n",
      "Epoch 10  |  Current Classification Error (%) 0.416679292296\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.37385376202\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.39062503716\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.457356303276\n",
      "Epoch 10  |  Cautious Classification Model Score 0.727309472492\n",
      "Epoch 20  |  Current Classification Error (%) 0.000780590460416\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.36797648831\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.36832275065\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.462016639195\n",
      "Epoch 20  |  Cautious Classification Model Score 0.73100533955\n",
      "Epoch 30  |  Current Classification Error (%) 0.000281799938929\n",
      "Epoch 30  |  Cautious Classification Calibration Perp 1.36795983827\n",
      "Epoch 30  |  Cautious Classification Model Perp 1.36825805159\n",
      "Epoch 30  |  Cautious Classification Calibration Score 0.462033336176\n",
      "Epoch 30  |  Cautious Classification Model Score 0.73101562866\n",
      "Test Classification Error (%) 1.66\n",
      "Test Cautious Classification Calibration Perp 1.3891\n",
      "Test Cautious Classification Model Perp 1.4917\n",
      "Test Cautious Classification Calibration Score 0.4469\n",
      "Test Cautious Classification Model Score 0.718406\n",
      "Epoch 10  |  Current Classification Error (%) 0.622645483511\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.37618056136\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.40128698743\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.455695356711\n",
      "Epoch 10  |  Cautious Classification Model Score 0.725811888692\n",
      "Epoch 20  |  Current Classification Error (%) 6.81468317733e-05\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.36794962297\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.36821193983\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.462042976158\n",
      "Epoch 20  |  Cautious Classification Model Score 0.731021042065\n",
      "Epoch 30  |  Current Classification Error (%) 0.00476900581373\n",
      "Epoch 30  |  Cautious Classification Calibration Perp 1.3680108517\n",
      "Epoch 30  |  Cautious Classification Model Perp 1.36839370513\n",
      "Epoch 30  |  Cautious Classification Calibration Score 0.461995886019\n",
      "Epoch 30  |  Cautious Classification Model Score 0.73098246707\n",
      "Test Classification Error (%) 1.5\n",
      "Test Cautious Classification Calibration Perp 1.3878\n",
      "Test Cautious Classification Model Perp 1.50388\n",
      "Test Cautious Classification Calibration Score 0.447997\n",
      "Test Cautious Classification Model Score 0.7196\n"
     ]
    }
   ],
   "source": [
    "train_and_test()\n",
    "train_and_test()\n",
    "train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  |  Current Classification Error (%) 0.492964803515\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.01624450795\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.03024725482\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.985736966119\n",
      "Epoch 10  |  Cautious Classification Model Score 0.991994368456\n",
      "Epoch 20  |  Current Classification Error (%) 0.000387889086603\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.00065019317\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.00108927638\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.998852761851\n",
      "Epoch 20  |  Cautious Classification Model Score 0.999425916576\n",
      "Epoch 30  |  Current Classification Error (%) 5.99268018009e-08\n",
      "Epoch 30  |  Cautious Classification Calibration Perp 1.00033587642\n",
      "Epoch 30  |  Cautious Classification Model Perp 1.00057627473\n",
      "Epoch 30  |  Cautious Classification Calibration Score 0.999371581918\n",
      "Epoch 30  |  Cautious Classification Model Score 0.999685801202\n",
      "Test Classification Error (%) 1.51\n",
      "Test Cautious Classification Calibration Perp 1.07178\n",
      "Test Cautious Classification Model Perp 1.15772\n",
      "Test Cautious Classification Calibration Score 0.968612\n",
      "Test Cautious Classification Model Score 0.983022\n",
      "Epoch 10  |  Current Classification Error (%) 0.385521686141\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.01345305793\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.02873438673\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.987455548967\n",
      "Epoch 10  |  Cautious Classification Model Score 0.993077165628\n",
      "Epoch 20  |  Current Classification Error (%) 8.81764889558e-06\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.00027256867\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.00052996979\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.999480506572\n",
      "Epoch 20  |  Cautious Classification Model Score 0.999740258354\n",
      "Epoch 30  |  Current Classification Error (%) 1.86084553124e-14\n",
      "Epoch 30  |  Cautious Classification Calibration Perp 1.00025121018\n",
      "Epoch 30  |  Cautious Classification Model Perp 1.00046695358\n",
      "Epoch 30  |  Cautious Classification Calibration Score 0.999518018019\n",
      "Epoch 30  |  Cautious Classification Model Score 0.99975901285\n",
      "Test Classification Error (%) 1.5\n",
      "Test Cautious Classification Calibration Perp 1.07125\n",
      "Test Cautious Classification Model Perp 1.15947\n",
      "Test Cautious Classification Calibration Score 0.969402\n",
      "Test Cautious Classification Model Score 0.983227\n",
      "Epoch 10  |  Current Classification Error (%) 0.622331476209\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.01745984271\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.03617324213\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.981872093257\n",
      "Epoch 10  |  Cautious Classification Model Score 0.989661248126\n",
      "Epoch 20  |  Current Classification Error (%) 8.3148174879e-05\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.00049382802\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.00091350948\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.999129851366\n",
      "Epoch 20  |  Cautious Classification Model Score 0.999564708541\n",
      "Epoch 30  |  Current Classification Error (%) 1.18056436581e-05\n",
      "Epoch 30  |  Cautious Classification Calibration Perp 1.00029681831\n",
      "Epoch 30  |  Cautious Classification Model Perp 1.00055416415\n",
      "Epoch 30  |  Cautious Classification Calibration Score 0.999438835167\n",
      "Epoch 30  |  Cautious Classification Model Score 0.999719374213\n",
      "Test Classification Error (%) 1.47\n",
      "Test Cautious Classification Calibration Perp 1.06906\n",
      "Test Cautious Classification Model Perp 1.15397\n",
      "Test Cautious Classification Calibration Score 0.968602\n",
      "Test Cautious Classification Model Score 0.982903\n"
     ]
    }
   ],
   "source": [
    "train_and_test(\"c_is_cotrained_sigmoid\")\n",
    "train_and_test(\"c_is_cotrained_sigmoid\")\n",
    "train_and_test(\"c_is_cotrained_sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  |  Current Classification Error (%) 0.473885899655\n",
      "Epoch 20  |  Current Classification Error (%) 0.00904122401227\n",
      "Epoch 30  |  Current Classification Error (%) 6.72494346557e-08\n",
      "Test Classification Error (%) 1.6\n",
      "Test Cautious Classification Calibration Perp 1.88311\n",
      "Test Cautious Classification Model Perp 2.04299\n",
      "Test Cautious Classification Calibration Score 0.967986\n",
      "Test Cautious Classification Model Score 0.983993\n"
     ]
    }
   ],
   "source": [
    "train_and_test(\"c_is_auxiliary_sigmoid\")\n",
    "# train_and_test(\"c_is_auxiliary_sigmoid\")\n",
    "# train_and_test(\"c_is_auxiliary_sigmoid\")\n",
    "\n",
    "# perhaps we need validation data or an aux ae"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
