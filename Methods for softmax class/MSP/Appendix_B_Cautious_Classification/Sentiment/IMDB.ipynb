{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import sklearn.metrics as sk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename='./data/imdb.train'):\n",
    "    '''\n",
    "    :param filename: the system location of the data to load\n",
    "    :return: the text (x) and its label (y)\n",
    "             the text is a list of words and is not processed\n",
    "    '''\n",
    "\n",
    "    # stop words taken from nltk\n",
    "    stop_words = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours',\n",
    "                  'yourself','yourselves','he','him','his','himself','she','her','hers','herself',\n",
    "                  'it','its','itself','they','them','their','theirs','themselves','what','which',\n",
    "                  'who','whom','this','that','these','those','am','is','are','was','were','be',\n",
    "                  'been','being','have','has','had','having','do','does','did','doing','a','an',\n",
    "                  'the','and','but','if','or','because','as','until','while','of','at','by','for',\n",
    "                  'with','about','against','between','into','through','during','before','after',\n",
    "                  'above','below','to','from','up','down','in','out','on','off','over','under',\n",
    "                  'again','further','then','once','here','there','when','where','why','how','all',\n",
    "                  'any','both','each','few','more','most','other','some','such','no','nor','not',\n",
    "                  'only','own','same','so','than','too','very','s','t','can','will','just','don',\n",
    "                  'should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn',\n",
    "                  'doesn','hadn','hasn','haven','isn','ma','mightn','mustn','needn','shan',\n",
    "                  'shouldn','wasn','weren','won','wouldn']\n",
    "\n",
    "    x, y = [], []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = re.sub(r'\\W+', ' ', line).strip().lower()  # perhaps don't make words lowercase?\n",
    "            x.append(line[:-1])\n",
    "            x[-1] = ' '.join(word for word in x[-1].split() if word not in stop_words)\n",
    "            y.append(line[-1])\n",
    "    return x, np.array(y, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(dataset):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "\n",
    "    :return: a _ordered_ dictionary from words to counts\n",
    "    '''\n",
    "    vocab = {}\n",
    "\n",
    "    # create a counter for each word\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] += 1\n",
    "    \n",
    "    # sort from greatest to least by count\n",
    "    return collections.OrderedDict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_rank(dataset, _vocab, desired_vocab_size=5000):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "    :vocab: a _ordered_ dictionary of vocab words and counts from get_vocab\n",
    "    :param desired_vocab_size: the desired vocabulary size\n",
    "    words no longer in vocab become UUUNNNKKK\n",
    "    :return: the text corpus with words mapped to their vocab rank,\n",
    "    with all sufficiently infrequent words mapped to UUUNNNKKK; UUUNNNKKK has rank desired_vocab_size\n",
    "    (the infrequent word cutoff is determined by desired_vocab size)\n",
    "    '''\n",
    "    _dataset = dataset[:]     # aliasing safeguard\n",
    "    vocab_ordered = list(_vocab)\n",
    "    count_cutoff = _vocab[vocab_ordered[desired_vocab_size-1]] # get word by its rank and map to its count\n",
    "    \n",
    "    word_to_rank = {}\n",
    "    for i in range(len(vocab_ordered)):\n",
    "        # we add one to make room for any future padding symbol with value 0\n",
    "        word_to_rank[vocab_ordered[i]] = i + 1\n",
    "    \n",
    "    # we need to ensure that other words below the word on the edge of our desired_vocab size\n",
    "    # are not also on the count cutoff, so we subtract a bit\n",
    "    # this is likely quicker than adding another preventative if case\n",
    "    for i in range(50):\n",
    "        _vocab[vocab_ordered[desired_vocab_size+i]] -= 0.1\n",
    "    \n",
    "    for i in range(len(_dataset)):\n",
    "        example = _dataset[i]\n",
    "        example_as_list = example.split()\n",
    "        for j in range(len(example_as_list)):\n",
    "            try:\n",
    "                if _vocab[example_as_list[j]] >= count_cutoff:\n",
    "                    example_as_list[j] = word_to_rank[example_as_list[j]] \n",
    "                else:\n",
    "                    example_as_list[j] = desired_vocab_size  # UUUNNNKKK\n",
    "            except:\n",
    "                example_as_list[j] = desired_vocab_size  # UUUNNNKKK\n",
    "        _dataset[i] = example_as_list\n",
    "\n",
    "    return _dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from keras\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "                  padding='pre', truncating='pre', value=0.):\n",
    "    '''Pads each sequence to the same length:\n",
    "    the length of the longest sequence.\n",
    "    If maxlen is provided, any sequence longer\n",
    "    than maxlen is truncated to maxlen.\n",
    "    Truncation happens off either the beginning (default) or\n",
    "    the end of the sequence.\n",
    "    Supports post-padding and pre-padding (default).\n",
    "    # Arguments\n",
    "        sequences: list of lists where each element is a sequence\n",
    "        maxlen: int, maximum length\n",
    "        dtype: type to cast the resulting sequence.\n",
    "        padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "        truncating: 'pre' or 'post', remove values from sequences larger than\n",
    "            maxlen either in the beginning or in the end of the sequence\n",
    "        value: float, value to pad the sequences to the desired value.\n",
    "    # Returns\n",
    "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
    "    '''\n",
    "    lengths = [len(s) for s in sequences]\n",
    "\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "max_example_len = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "vocab_size = 5000\n",
    "training_epochs = 20\n",
    "\n",
    "print('Loading Data')\n",
    "X_train, Y_train = load_data()\n",
    "X_dev, Y_dev = load_data('./data/imdb.dev')\n",
    "X_test, Y_test = load_data('./data/imdb.test')\n",
    "\n",
    "vocab = get_vocab(X_train)\n",
    "X_train = text_to_rank(X_train, vocab, 5000)\n",
    "X_dev = text_to_rank(X_dev, vocab, 5000)\n",
    "X_test = text_to_rank(X_test, vocab, 5000)\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen=max_example_len)\n",
    "X_dev = pad_sequences(X_dev, maxlen=max_example_len)\n",
    "X_test = pad_sequences(X_test, maxlen=max_example_len)\n",
    "print('Data loaded')\n",
    "\n",
    "num_examples = Y_train.shape[0]\n",
    "num_batches = num_examples//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test(mode=\"c_is_softmax_prob\", seed=100, learning_rate=0.001,\n",
    "                   X_train=X_train, Y_train=Y_train, X_test=X_test, Y_test=Y_test):\n",
    "    '''\n",
    "    modes: c_is_softmax_prob, c_is_trained_softmax_prob, c_is_cotrained_sigmoid, c_is_auxiliary_sigmoid\n",
    "    '''\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)  # seed set upon graph construction; does not work\n",
    "        \n",
    "        x = tf.placeholder(dtype=tf.int32, shape=[None, max_example_len])\n",
    "        y = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "\n",
    "        def gelu(x):\n",
    "            return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "        f = gelu\n",
    "\n",
    "        W = {}\n",
    "        b = {}\n",
    "\n",
    "        with tf.variable_scope(\"classifier\"):\n",
    "            W['embedding'] = tf.Variable(tf.nn.l2_normalize(\n",
    "                tf.random_normal([vocab_size+1, embedding_dims]), 0), trainable=True)\n",
    "            W['logits'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([embedding_dims, 2]), 0))\n",
    "            \n",
    "            b['logits'] = tf.Variable(tf.zeros([2]))\n",
    "\n",
    "        with tf.variable_scope(\"confidence_scorer\"):\n",
    "            W['hidden_to_conf1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal(\n",
    "                        [embedding_dims, embedding_dims//2]), 0))\n",
    "            W['logits_to_conf1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([2, embedding_dims//2]), 0))\n",
    "            W['conf'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([embedding_dims//2, 1]), 0))\n",
    "\n",
    "            b['conf1'] = tf.Variable(tf.zeros([embedding_dims//2]))\n",
    "            b['conf'] = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "        def cautious_fcn(x):\n",
    "            w_vecs = tf.nn.embedding_lookup(W['embedding'], x)\n",
    "            pooled = tf.reduce_mean(w_vecs, reduction_indices=[1])\n",
    "\n",
    "            logits_out = tf.matmul(pooled, W['logits']) + b['logits']\n",
    "\n",
    "            conf1 = f(tf.matmul(logits_out, W['logits_to_conf1']) +\n",
    "                        tf.matmul(pooled, W['hidden_to_conf1']) + b['conf1'])\n",
    "            conf_out = tf.matmul(conf1, W['conf']) + b['conf']\n",
    "\n",
    "            return logits_out, tf.squeeze(conf_out)\n",
    "\n",
    "        logits, confidence_logit = cautious_fcn(x)\n",
    "\n",
    "        right_answer = tf.stop_gradient(tf.to_float(tf.equal(tf.argmax(logits, 1), y)))\n",
    "        compute_error = 100*tf.reduce_mean(1 - right_answer)\n",
    "\n",
    "        classification_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "        if \"softmax\" in mode:\n",
    "            confidence_logit = tf.reduce_max(tf.nn.softmax(logits), reduction_indices=[1])\n",
    "            caution_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(confidence_logit, right_answer))\n",
    "            \n",
    "            # cc_loss is cautious classification loss\n",
    "            if mode == \"c_is_trained_softmax_prob\":\n",
    "                cc_loss = classification_loss + caution_loss\n",
    "            else:\n",
    "                cc_loss = classification_loss\n",
    "        \n",
    "        elif mode == \"c_is_cotrained_sigmoid\":\n",
    "            caution_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(confidence_logit, right_answer))\n",
    "            cc_loss = classification_loss + caution_loss\n",
    "            confidence = tf.sigmoid(confidence_logit)\n",
    "        elif mode == \"c_is_auxiliary_sigmoid\":\n",
    "            caution_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(confidence_logit, right_answer))\n",
    "            cc_loss = classification_loss  # we use caution_loss after training normal classifier\n",
    "        else:\n",
    "            assert False, \"Invalid mode specified\"\n",
    "        \n",
    "        cc_calibration_score = tf.reduce_mean((2 * right_answer - 1) * (2 * tf.sigmoid(confidence_logit) - 1))\n",
    "        cc_model_score = tf.reduce_mean(right_answer * ((2 * right_answer - 1) * (2 * tf.sigmoid(confidence_logit) - 1)+ 1)/2)\n",
    "        \n",
    "        # cautious classification perplexity\n",
    "        cc_calibration_perplexity = tf.exp(caution_loss)\n",
    "        cc_model_perplexity = tf.exp(caution_loss + classification_loss)\n",
    "        \n",
    "        lr = tf.constant(learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cc_loss)\n",
    "\n",
    "    sess = tf.InteractiveSession(graph=graph)\n",
    "    \n",
    "    if \"softmax\" in mode:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    elif mode == \"c_is_cotrained_sigmoid\":\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    elif mode == \"c_is_auxiliary_sigmoid\":\n",
    "        thawed_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"classifier\")\n",
    "        frozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"confidence_scorer\")\n",
    "        sess.run(tf.initialize_variables(set(tf.all_variables()) - set(frozen_vars)))\n",
    "    \n",
    "    err_ema = 90\n",
    "    cc_calibration_perp_ema = 10\n",
    "    cc_model_perp_ema = 10\n",
    "    cc_calibration_score_ema = -1\n",
    "    cc_model_score_ema = -1\n",
    "    \n",
    "    for epoch in range(1,training_epochs+1):\n",
    "        # shuffle data every epoch\n",
    "        indices = np.arange(num_examples)\n",
    "        np.random.shuffle(indices)\n",
    "        X_train = X_train[indices]\n",
    "        Y_train = Y_train[indices]\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            offset = i * batch_size\n",
    "            bx = X_train[offset:offset + batch_size]\n",
    "            by = Y_train[offset:offset + batch_size]\n",
    "\n",
    "            if mode != \"c_is_auxiliary_sigmoid\":\n",
    "\n",
    "                _, err, cc_model_score_curr, cc_calibration_score_curr,\\\n",
    "                cc_model_perp_curr, cc_calibration_perp_curr = sess.run([\n",
    "                        optimizer, compute_error, cc_model_score, cc_calibration_score,\n",
    "                        cc_model_perplexity, cc_calibration_perplexity],\n",
    "                     feed_dict={x: bx, y: by, lr: learning_rate})\n",
    "\n",
    "                err_ema = err_ema * 0.95 + 0.05 * err\n",
    "                cc_calibration_perp_ema = cc_calibration_perp_ema * 0.95 + 0.05 * cc_calibration_perp_curr\n",
    "                cc_model_perp_ema = cc_model_perp_ema * 0.95 + 0.05 * cc_model_perp_curr\n",
    "                cc_calibration_score_ema = cc_calibration_score_ema * 0.95 + 0.05 * cc_calibration_score_curr\n",
    "                cc_model_score_ema = cc_model_score_ema * 0.95 + 0.05 * cc_model_score_curr\n",
    "            else:\n",
    "                _, err, l = sess.run([optimizer, compute_error, cc_loss],\n",
    "                                     feed_dict={x: bx, y: by, lr: learning_rate})\n",
    "                err_ema = err_ema * 0.95 + 0.05 * err\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch', epoch, ' | ', 'Current Classification Error (%)', err_ema)\n",
    "            if mode != \"c_is_auxiliary_sigmoid\":\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Calibration Perp', cc_calibration_perp_ema)\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Model Perp', cc_model_perp_ema)\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Calibration Score', cc_calibration_score_ema)\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Model Score', cc_model_score_ema)\n",
    "\n",
    "    if mode == \"c_is_auxiliary_sigmoid\":\n",
    "        # train sigmoid separately from the classifier\n",
    "        phase2_vars = list(set(tf.all_variables()) - set(thawed_vars))\n",
    "        optimizer2 = tf.train.AdamOptimizer(learning_rate=0.001).minimize(caution_loss, var_list=phase2_vars)\n",
    "        sess.run(tf.initialize_variables(set(tf.all_variables()) - set(thawed_vars)))\n",
    "        \n",
    "        for epoch in range(2):\n",
    "            # shuffle data every epoch\n",
    "            indices = np.arange(num_examples)\n",
    "            np.random.shuffle(indices)\n",
    "            X_train = X_train[indices]\n",
    "            Y_train = Y_train[indices]\n",
    "\n",
    "            for i in range(num_batches):\n",
    "                bx = X_train[offset:offset + batch_size]\n",
    "                by = Y_train[offset:offset + batch_size]\n",
    "\n",
    "                sess.run([optimizer2], feed_dict={x: bx, y: by})\n",
    "\n",
    "    err, cc_model_score_test, cc_calibration_score_test,\\\n",
    "    cc_model_perp_test, cc_calibration_perp_test = sess.run([\n",
    "                    compute_error, cc_model_score, cc_calibration_score,\n",
    "                    cc_model_perplexity, cc_calibration_perplexity],\n",
    "                                  feed_dict={x: X_test, y: Y_test})\n",
    "\n",
    "    print('Test Classification Error (%)', err)\n",
    "    print('Test Cautious Classification Calibration Perp', cc_calibration_perp_test)\n",
    "    print('Test Cautious Classification Model Perp', cc_model_perp_test)\n",
    "    print('Test Cautious Classification Calibration Score', cc_calibration_score_test)\n",
    "    print('Test Cautious Classification Model Score', cc_model_score_test)\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  |  Current Classification Error (%) 8.04213016869\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.4913142399\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.84001342238\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.366954381777\n",
      "Epoch 10  |  Cautious Classification Model Score 0.656664406814\n",
      "Test Classification Error (%) 12.3\n",
      "Test Cautious Classification Calibration Perp 1.54706\n",
      "Test Cautious Classification Model Perp 2.1239\n",
      "Test Cautious Classification Calibration Score 0.331525\n",
      "Test Cautious Classification Model Score 0.626295\n",
      "Epoch 10  |  Current Classification Error (%) 9.01776217802\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.5067733706\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.93401051969\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.35768268921\n",
      "Epoch 10  |  Cautious Classification Model Score 0.649408129246\n",
      "Test Classification Error (%) 12.968\n",
      "Test Cautious Classification Calibration Perp 1.55451\n",
      "Test Cautious Classification Model Perp 2.16295\n",
      "Test Cautious Classification Calibration Score 0.327657\n",
      "Test Cautious Classification Model Score 0.622469\n",
      "Epoch 10  |  Current Classification Error (%) 8.74696441255\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.49859240673\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.85765795816\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.362969619449\n",
      "Epoch 10  |  Cautious Classification Model Score 0.652586620277\n",
      "Test Classification Error (%) 13.996\n",
      "Test Cautious Classification Calibration Perp 1.57026\n",
      "Test Cautious Classification Model Perp 2.23955\n",
      "Test Cautious Classification Calibration Score 0.317181\n",
      "Test Cautious Classification Model Score 0.613974\n"
     ]
    }
   ],
   "source": [
    "train_and_test()\n",
    "train_and_test()\n",
    "train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  |  Current Classification Error (%) 9.06342513393\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.26285863247\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.61074784375\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.727877072048\n",
      "Epoch 10  |  Cautious Classification Model Score 0.839024624966\n",
      "Test Classification Error (%) 11.968\n",
      "Test Cautious Classification Calibration Perp 1.34651\n",
      "Test Cautious Classification Model Perp 1.81795\n",
      "Test Cautious Classification Calibration Score 0.678708\n",
      "Test Cautious Classification Model Score 0.810641\n",
      "Epoch 10  |  Current Classification Error (%) 8.52237364392\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.24786365961\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.59730933445\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.729640300553\n",
      "Epoch 10  |  Cautious Classification Model Score 0.842416382632\n",
      "Test Classification Error (%) 12.056\n",
      "Test Cautious Classification Calibration Perp 1.35883\n",
      "Test Cautious Classification Model Perp 1.837\n",
      "Test Cautious Classification Calibration Score 0.692414\n",
      "Test Cautious Classification Model Score 0.818819\n",
      "Epoch 10  |  Current Classification Error (%) 10.1644734978\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.31099227112\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.75256107516\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.675811019478\n",
      "Epoch 10  |  Cautious Classification Model Score 0.808336328174\n",
      "Test Classification Error (%) 12.1\n",
      "Test Cautious Classification Calibration Perp 1.35823\n",
      "Test Cautious Classification Model Perp 1.83806\n",
      "Test Cautious Classification Calibration Score 0.689054\n",
      "Test Cautious Classification Model Score 0.817251\n"
     ]
    }
   ],
   "source": [
    "train_and_test(\"c_is_cotrained_sigmoid\")\n",
    "train_and_test(\"c_is_cotrained_sigmoid\")\n",
    "train_and_test(\"c_is_cotrained_sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  |  Current Classification Error (%) 9.82396211464\n",
      "Test Classification Error (%) 24.164\n",
      "Test Cautious Classification Calibration Perp 1.4518\n",
      "Test Cautious Classification Model Perp 2.6351\n",
      "Test Cautious Classification Calibration Score 0.648539\n",
      "Test Cautious Classification Model Score 0.655682\n"
     ]
    }
   ],
   "source": [
    "train_and_test(\"c_is_auxiliary_sigmoid\")\n",
    "# looks like it needs some regularization\n",
    "# it probably has learned to guess extremely and positively\n",
    "# or perhaps it just doesn't decreased accuracy like the cotrained sigmoid may\n",
    "# so maybe change emphasis on cotrained sigmoid"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
