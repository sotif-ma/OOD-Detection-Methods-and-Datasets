{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pylab as plt\n",
    "from helper_functions_twitter import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window_size = 1\n",
    "\n",
    "# note that we encode the tags with numbers for later convenience\n",
    "tag_to_number = {\n",
    "    u'N': 0, u'O': 1, u'S': 2, u'^': 3, u'Z': 4, u'L': 5, u'M': 6,\n",
    "    u'V': 7, u'A': 8, u'R': 9, u'!': 10, u'D': 11, u'P': 12, u'&': 13, u'T': 14,\n",
    "    u'X': 15, u'Y': 16, u'#': 17, u'@': 18, u'~': 19, u'U': 20, u'E': 21, u'$': 22,\n",
    "    u',': 23, u'G': 24\n",
    "}\n",
    "\n",
    "embeddings = embeddings_to_dict('./data/Tweets/embeddings-twitter.txt')\n",
    "vocab = embeddings.keys()\n",
    "\n",
    "# we replace <s> with </s> since it has no embedding, and </s> is a better embedding than UNK\n",
    "xt, yt = data_to_mat('./data/Tweets/tweets-train.txt', vocab, tag_to_number, window_size=window_size,\n",
    "                     start_symbol=u'</s>', one_hot=True)\n",
    "xdev, ydev = data_to_mat('./data/Tweets/tweets-dev.txt', vocab, tag_to_number, window_size=window_size,\n",
    "                         start_symbol=u'</s>', one_hot=True)\n",
    "xdtest, ydtest = data_to_mat('./data/Tweets/tweets-devtest.txt', vocab, tag_to_number, window_size=window_size,\n",
    "                             start_symbol=u'</s>', one_hot=True)\n",
    "\n",
    "data = {\n",
    "    'x_train': xt, 'y_train': yt,\n",
    "    'x_dev': xdev, 'y_dev': ydev,\n",
    "    'x_devtest': xdtest, 'y_devtest': ydtest\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_and_test(mode=\"c_is_softmax_prob\", seed=100, learning_rate=0.001):\n",
    "    \n",
    "    training_epochs = 20\n",
    "    n_labels = 25\n",
    "    batch_size = 64\n",
    "    embedding_dimension = 50\n",
    "    example_size = (2*window_size + 1)*embedding_dimension\n",
    "    num_examples = data['y_train'].shape[0]\n",
    "    num_batches = num_examples//batch_size\n",
    "    \n",
    "    '''\n",
    "    modes: c_is_softmax_prob, c_is_trained_softmax_prob, c_is_cotrained_sigmoid, c_is_auxiliary_sigmoid\n",
    "    '''\n",
    "    \n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        tf.set_random_seed(seed)  # seed set upon graph construction; does not work\n",
    "\n",
    "        x = tf.placeholder(dtype=tf.float32, shape=[None, example_size])\n",
    "        y = tf.placeholder(dtype=tf.float32, shape=[None, n_labels])\n",
    "\n",
    "        def gelu(x):\n",
    "            return 0.5 * x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3))))\n",
    "        f = gelu\n",
    "\n",
    "        W = {}\n",
    "        b = {}\n",
    "\n",
    "        with tf.variable_scope(\"classifier\"):\n",
    "            W['1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([example_size, 256]), 0))\n",
    "            W['2'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([256, 256]), 0))\n",
    "            W['3'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([256, 256]), 0))\n",
    "            W['logits'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([256, n_labels]), 0))\n",
    "\n",
    "            b['1'] = tf.Variable(tf.zeros([256]))\n",
    "            b['2'] = tf.Variable(tf.zeros([256]))\n",
    "            b['3'] = tf.Variable(tf.zeros([256]))\n",
    "            b['logits'] = tf.Variable(tf.zeros([n_labels]))\n",
    "\n",
    "        with tf.variable_scope(\"confidence_scorer\"):\n",
    "            W['hidden_to_conf1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([256, 512]), 0))\n",
    "            W['logits_to_conf1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_labels, 512]), 0))\n",
    "            W['conf2'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([512, 128]), 0))\n",
    "            W['conf'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([128, 1]), 0))\n",
    "\n",
    "            b['conf1'] = tf.Variable(tf.zeros([512]))\n",
    "            b['conf2'] = tf.Variable(tf.zeros([128]))\n",
    "            b['conf'] = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "        def cautious_fcn(x):\n",
    "            h1 = f(tf.matmul(x, W['1']) + b['1'])\n",
    "            h2 = f(tf.matmul(h1, W['2']) + b['2'])\n",
    "            h3 = f(tf.matmul(h2, W['3']) + b['3'])\n",
    "            logits_out = tf.matmul(h3, W['logits']) + b['logits']\n",
    "\n",
    "            conf1 = f(tf.matmul(logits_out, W['logits_to_conf1']) +\n",
    "                        tf.matmul(h2, W['hidden_to_conf1']) + b['conf1'])\n",
    "            conf2 = f(tf.matmul(conf1, W['conf2']) + b['conf2'])\n",
    "            conf_out = tf.matmul(conf2, W['conf']) + b['conf']\n",
    "\n",
    "            return logits_out, tf.squeeze(conf_out)\n",
    "\n",
    "        logits, confidence_logit = cautious_fcn(x)\n",
    "\n",
    "        right_answer = tf.stop_gradient(tf.to_float(tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))))\n",
    "        compute_error = 100*tf.reduce_mean(1 - right_answer)\n",
    "\n",
    "        classification_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, y))\n",
    "        if \"softmax\" in mode:\n",
    "            confidence_logit = tf.reduce_max(tf.nn.softmax(logits), reduction_indices=[1])\n",
    "            caution_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(confidence_logit, right_answer))\n",
    "            \n",
    "            # cc_loss is cautious classification loss\n",
    "            if mode == \"c_is_trained_softmax_prob\":\n",
    "                cc_loss = classification_loss + caution_loss\n",
    "            else:\n",
    "                cc_loss = classification_loss\n",
    "        \n",
    "        elif mode == \"c_is_cotrained_sigmoid\":\n",
    "            caution_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(confidence_logit, right_answer))\n",
    "            cc_loss = classification_loss + caution_loss\n",
    "            confidence = tf.sigmoid(confidence_logit)\n",
    "        elif mode == \"c_is_auxiliary_sigmoid\":\n",
    "            caution_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(confidence_logit, right_answer))\n",
    "            cc_loss = classification_loss  # we use caution_loss after training normal classifier\n",
    "        else:\n",
    "            assert False, \"Invalid mode specified\"\n",
    "        \n",
    "        cc_calibration_score = tf.reduce_mean((2 * right_answer - 1) * (2 * tf.sigmoid(confidence_logit) - 1))\n",
    "        cc_model_score = tf.reduce_mean(right_answer * ((2 * right_answer - 1) * (2 * tf.sigmoid(confidence_logit) - 1)+ 1)/2)\n",
    "        \n",
    "        # cautious classification perplexity\n",
    "        cc_calibration_perplexity = tf.exp(caution_loss)\n",
    "        cc_model_perplexity = tf.exp(caution_loss + classification_loss)\n",
    "        \n",
    "        lr = tf.constant(learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(cc_loss)\n",
    "\n",
    "    sess = tf.InteractiveSession(graph=graph)\n",
    "    \n",
    "    if \"softmax\" in mode:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    elif mode == \"c_is_cotrained_sigmoid\":\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    elif mode == \"c_is_auxiliary_sigmoid\":\n",
    "        thawed_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"classifier\")\n",
    "        frozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"confidence_scorer\")\n",
    "        sess.run(tf.initialize_variables(set(tf.all_variables()) - set(frozen_vars)))\n",
    "\n",
    "    err_ema = 90\n",
    "    cc_calibration_perp_ema = 10\n",
    "    cc_model_perp_ema = 10\n",
    "    cc_calibration_score_ema = -1\n",
    "    cc_model_score_ema = -1\n",
    "\n",
    "    for epoch in range(1,training_epochs+1):\n",
    "        # shuffle data\n",
    "        indices = np.arange(num_examples)\n",
    "        np.random.shuffle(indices)\n",
    "        data['x_train'] = data['x_train'][indices]\n",
    "        data['y_train'] = data['y_train'][indices]\n",
    "\n",
    "        for i in range(num_batches):\n",
    "            offset = i * batch_size\n",
    "\n",
    "            bx = word_list_to_embedding(data['x_train'][offset:offset + batch_size, :],\n",
    "                                                 embeddings, embedding_dimension)\n",
    "            by = data['y_train'][offset:offset + batch_size]\n",
    "\n",
    "            if mode != \"c_is_auxiliary_sigmoid\":\n",
    "                _, err, cc_model_score_curr, cc_calibration_score_curr,\\\n",
    "                cc_model_perp_curr, cc_calibration_perp_curr = sess.run([\n",
    "                        optimizer, compute_error, cc_model_score, cc_calibration_score,\n",
    "                        cc_model_perplexity, cc_calibration_perplexity],\n",
    "                     feed_dict={x: bx, y: by, lr: learning_rate})\n",
    "\n",
    "                err_ema = err_ema * 0.95 + 0.05 * err\n",
    "                cc_calibration_perp_ema = cc_calibration_perp_ema * 0.95 + 0.05 * cc_calibration_perp_curr\n",
    "                cc_model_perp_ema = cc_model_perp_ema * 0.95 + 0.05 * cc_model_perp_curr\n",
    "                cc_calibration_score_ema = cc_calibration_score_ema * 0.95 + 0.05 * cc_calibration_score_curr\n",
    "                cc_model_score_ema = cc_model_score_ema * 0.95 + 0.05 * cc_model_score_curr\n",
    "            else:\n",
    "                _, err = sess.run([optimizer, compute_error],\n",
    "                                  feed_dict={x: bx, y: by, lr: learning_rate})\n",
    "                err_ema = err_ema * 0.95 + 0.05 * err\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print('Epoch', epoch, ' | ', 'Current Classification Error (%)', err_ema)\n",
    "            if mode != \"c_is_auxiliary_sigmoid\":\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Calibration Perp', cc_calibration_perp_ema)\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Model Perp', cc_model_perp_ema)\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Calibration Score', cc_calibration_score_ema)\n",
    "                print('Epoch', epoch, ' | ', 'Cautious Classification Model Score', cc_model_score_ema)\n",
    "\n",
    "    if mode == \"c_is_auxiliary_sigmoid\":\n",
    "        # train sigmoid separately from the classifier\n",
    "        phase2_vars = list(set(tf.all_variables()) - set(thawed_vars))\n",
    "        optimizer2 = tf.train.AdamOptimizer(learning_rate=0.001).minimize(caution_loss, var_list=phase2_vars)\n",
    "        sess.run(tf.initialize_variables(set(tf.all_variables()) - set(thawed_vars)))\n",
    "\n",
    "        for epoch in range(3):\n",
    "            for i in range(num_batches):\n",
    "                offset = i * batch_size\n",
    "\n",
    "                bx = word_list_to_embedding(data['x_train'][offset:offset + batch_size, :],\n",
    "                                                     embeddings, embedding_dimension)\n",
    "                by = data['y_train'][offset:offset + batch_size]\n",
    "\n",
    "                sess.run([optimizer2], feed_dict={x: bx, y: by})\n",
    "\n",
    "    err, cc_model_score_test, cc_calibration_score_test,\\\n",
    "    cc_model_perp_test, cc_calibration_perp_test = sess.run([\n",
    "                    compute_error, cc_model_score, cc_calibration_score,\n",
    "                    cc_model_perplexity, cc_calibration_perplexity],\n",
    "    feed_dict={x: word_list_to_embedding(data['x_devtest'], embeddings, embedding_dimension),\n",
    "               y: data['y_devtest']})\n",
    "\n",
    "    print('Test Classification Error (%)', err)\n",
    "    print('Test Cautious Classification Calibration Perp', cc_calibration_perp_test)\n",
    "    print('Test Cautious Classification Model Perp', cc_model_perp_test)\n",
    "    print('Test Cautious Classification Calibration Score', cc_calibration_score_test)\n",
    "    print('Test Cautious Classification Model Score', cc_model_score_test)\n",
    "\n",
    "    sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  |  Current Classification Error (%) 7.64026743057\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.46547546575\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.85142755641\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.386193511063\n",
      "Epoch 10  |  Cautious Classification Model Score 0.665618254806\n",
      "Epoch 20  |  Current Classification Error (%) 1.56097880302\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.39106289344\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.47106011707\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.443357149912\n",
      "Epoch 20  |  Cautious Classification Model Score 0.716329939717\n",
      "Test Classification Error (%) 13.7304\n",
      "Test Cautious Classification Calibration Perp 1.5507\n",
      "Test Cautious Classification Model Perp 3.51644\n",
      "Test Cautious Classification Calibration Score 0.337601\n",
      "Test Cautious Classification Model Score 0.626207\n",
      "Epoch 10  |  Current Classification Error (%) 6.52178477683\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.45401980373\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.81153794018\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.394639810404\n",
      "Epoch 10  |  Cautious Classification Model Score 0.673948091463\n",
      "Epoch 20  |  Current Classification Error (%) 2.4737086154\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.40150844649\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.51113015682\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.435446983213\n",
      "Epoch 20  |  Cautious Classification Model Score 0.709165153082\n",
      "Test Classification Error (%) 13.9681\n",
      "Test Cautious Classification Calibration Perp 1.55188\n",
      "Test Cautious Classification Model Perp 3.64268\n",
      "Test Cautious Classification Calibration Score 0.337287\n",
      "Test Cautious Classification Model Score 0.625224\n",
      "Epoch 10  |  Current Classification Error (%) 7.5293218826\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.46495451536\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.8431294227\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.386694858861\n",
      "Epoch 10  |  Cautious Classification Model Score 0.666371045987\n",
      "Epoch 20  |  Current Classification Error (%) 2.00522231909\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.39658729767\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.49277278706\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.43909283851\n",
      "Epoch 20  |  Cautious Classification Model Score 0.712649607651\n",
      "Test Classification Error (%) 14.2198\n",
      "Test Cautious Classification Calibration Perp 1.55474\n",
      "Test Cautious Classification Model Perp 3.61556\n",
      "Test Cautious Classification Calibration Score 0.33516\n",
      "Test Cautious Classification Model Score 0.623195\n"
     ]
    }
   ],
   "source": [
    "train_and_test()\n",
    "train_and_test()\n",
    "train_and_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  |  Current Classification Error (%) 7.48396218212\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.1575132031\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.46652571859\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.820281630018\n",
      "Epoch 10  |  Cautious Classification Model Score 0.88133875657\n",
      "Epoch 20  |  Current Classification Error (%) 2.64868468146\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.07687996985\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.17345266493\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.920370430996\n",
      "Epoch 20  |  Cautious Classification Model Score 0.951495754674\n",
      "Test Classification Error (%) 13.9402\n",
      "Test Cautious Classification Calibration Perp 1.57654\n",
      "Test Cautious Classification Model Perp 3.31664\n",
      "Test Cautious Classification Calibration Score 0.720826\n",
      "Test Cautious Classification Model Score 0.84076\n",
      "Epoch 10  |  Current Classification Error (%) 7.2899537807\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.16116282811\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.46671502571\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.823509813979\n",
      "Epoch 10  |  Cautious Classification Model Score 0.883757523806\n",
      "Epoch 20  |  Current Classification Error (%) 1.66209556079\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.04735118208\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.11922261748\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.94650470986\n",
      "Epoch 20  |  Cautious Classification Model Score 0.968646617144\n",
      "Test Classification Error (%) 13.0872\n",
      "Test Cautious Classification Calibration Perp 1.9009\n",
      "Test Cautious Classification Model Perp 3.9842\n",
      "Test Cautious Classification Calibration Score 0.744274\n",
      "Test Cautious Classification Model Score 0.860583\n",
      "Epoch 10  |  Current Classification Error (%) 7.69865044825\n",
      "Epoch 10  |  Cautious Classification Calibration Perp 1.17741953998\n",
      "Epoch 10  |  Cautious Classification Model Perp 1.50496898407\n",
      "Epoch 10  |  Cautious Classification Calibration Score 0.801836216427\n",
      "Epoch 10  |  Cautious Classification Model Score 0.872736561717\n",
      "Epoch 20  |  Current Classification Error (%) 2.00094767709\n",
      "Epoch 20  |  Cautious Classification Calibration Perp 1.06933431412\n",
      "Epoch 20  |  Cautious Classification Model Perp 1.14531232709\n",
      "Epoch 20  |  Cautious Classification Calibration Score 0.932037780647\n",
      "Epoch 20  |  Cautious Classification Model Score 0.961318182405\n",
      "Test Classification Error (%) 13.5766\n",
      "Test Cautious Classification Calibration Perp 1.6671\n",
      "Test Cautious Classification Model Perp 3.48629\n",
      "Test Cautious Classification Calibration Score 0.731238\n",
      "Test Cautious Classification Model Score 0.851706\n"
     ]
    }
   ],
   "source": [
    "train_and_test(\"c_is_cotrained_sigmoid\")\n",
    "train_and_test(\"c_is_cotrained_sigmoid\")\n",
    "train_and_test(\"c_is_cotrained_sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10  |  Current Classification Error (%) 6.30479048276\n",
      "Epoch 20  |  Current Classification Error (%) 2.46373553995\n",
      "Test Classification Error (%) 13.8003\n",
      "Test Cautious Classification Calibration Perp 3.28663\n",
      "Test Cautious Classification Model Perp 7.52278\n",
      "Test Cautious Classification Calibration Score 0.725052\n",
      "Test Cautious Classification Model Score 0.860351\n"
     ]
    }
   ],
   "source": [
    "train_and_test(\"c_is_auxiliary_sigmoid\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
