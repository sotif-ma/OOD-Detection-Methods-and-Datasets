{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code sets up the model, evaluates the effectiveness of softmax information alone, and after shows the improvement gained from an abnormality module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 20\n",
    "batch_size = 32\n",
    "\n",
    "# architecture parameters\n",
    "n_hidden = 1024\n",
    "n_labels = 39   # 39 phones\n",
    "n_coeffs = 26\n",
    "n_context_frames = 11   # 5 + 1 + 5\n",
    "p = 0.75        # keep rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def enumerate_context(i, sentence, num_frames):\n",
    "    r = range(i-num_frames, i+num_frames+1)\n",
    "    r = [x if x>=0 else 0 for x in r]\n",
    "    r = [x if x<len(sentence) else len(sentence)-1 for x in r]\n",
    "    return sentence[r]\n",
    "\n",
    "def add_context(sentence, num_frames=11):\n",
    "    # [sentence_length, coefficients] -> [sentence_length, num_frames, coefficients]\n",
    "\n",
    "    assert num_frames % 2 == 1, \"Number of frames must be odd (since left + 1 + right, left = right)\"\n",
    "\n",
    "    if num_frames == 1:\n",
    "        return sentence\n",
    "\n",
    "    context_sent = []\n",
    "\n",
    "    for i in range(0, len(sentence)):\n",
    "        context_sent.append([context for context in enumerate_context(i, sentence, (num_frames-1)//2)])\n",
    "\n",
    "    return np.array(context_sent).reshape((-1, num_frames*n_coeffs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, n_coeffs*n_context_frames])\n",
    "    y = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "    risk_labels = tf.placeholder(dtype=tf.float32, shape=[None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "    # nonlinearity\n",
    "    def gelu_fast(_x):\n",
    "        return 0.5 * _x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (_x + 0.044715 * tf.pow(_x, 3))))\n",
    "    f = gelu_fast\n",
    "\n",
    "    W = {}\n",
    "    b = {}\n",
    "\n",
    "    with tf.variable_scope(\"in_sample\"):\n",
    "        W['1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_context_frames*n_coeffs, n_hidden]), 0)/tf.sqrt(1 + p*0.425))\n",
    "        W['2'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_hidden, n_hidden]), 0)/tf.sqrt(0.425/p + p*0.425))\n",
    "        W['3'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_hidden, n_hidden]), 0)/tf.sqrt(0.425/p + p*0.425))\n",
    "        W['logits'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_hidden, n_labels]), 0)/tf.sqrt(0.425/p + 1))\n",
    "        b['1'] = tf.Variable(tf.zeros([n_hidden]))\n",
    "        b['2'] = tf.Variable(tf.zeros([n_hidden]))\n",
    "        b['3'] = tf.Variable(tf.zeros([n_hidden]))\n",
    "        b['logits'] = tf.Variable(tf.zeros([n_labels]))\n",
    "\n",
    "        W['bottleneck'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_hidden, n_hidden//2]), 0)/tf.sqrt(0.425/p + 0.425))\n",
    "        W['decode1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_hidden//2, n_hidden]), 0)/tf.sqrt(0.425 + p*0.425))\n",
    "        W['decode2'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_hidden, n_hidden]), 0)/tf.sqrt(0.425/p + 0.425*p))\n",
    "        W['reconstruction'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_hidden, n_context_frames*n_coeffs]), 0)/tf.sqrt(0.425/p + 1))\n",
    "        b['bottleneck'] = tf.Variable(tf.zeros([n_hidden//2]))\n",
    "        b['decode1'] = tf.Variable(tf.zeros([n_hidden]))\n",
    "        b['decode2'] = tf.Variable(tf.zeros([n_hidden]))\n",
    "        b['reconstruction'] = tf.Variable(tf.zeros([n_context_frames*n_coeffs]))\n",
    "\n",
    "    with tf.variable_scope(\"out_of_sample\"):\n",
    "        W['residual_to_risk1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_context_frames*n_coeffs, n_hidden//2]), 0)/tf.sqrt(1 + 0.425))\n",
    "        W['hidden_to_risk1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_hidden, n_hidden//2]), 0)/tf.sqrt(0.425/p + 0.425))\n",
    "        W['logits_to_risk1'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_labels, n_hidden//2]), 0)/tf.sqrt(1 + 0.425))\n",
    "        W['risk2'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_hidden//2, 128]), 0)/tf.sqrt(0.425 + 0.425))\n",
    "        W['risk'] = tf.Variable(tf.nn.l2_normalize(tf.random_normal([128, 1]), 0)/tf.sqrt(0.425 + 1))\n",
    "\n",
    "        b['risk1'] = tf.Variable(tf.zeros([n_hidden//2]))\n",
    "        b['risk2'] = tf.Variable(tf.zeros([128]))\n",
    "        b['risk'] = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "    def feedforward(x):\n",
    "        h1 = f(tf.matmul(x, W['1']) + b['1'])\n",
    "        h1 = tf.cond(is_training, lambda: tf.nn.dropout(h1, p), lambda: h1)\n",
    "        h2 = f(tf.matmul(h1, W['2']) + b['2'])\n",
    "        h2 = tf.cond(is_training, lambda: tf.nn.dropout(h2, p), lambda: h2)\n",
    "        h3 = f(tf.matmul(h2, W['3']) + b['3'])\n",
    "        h3 = tf.cond(is_training, lambda: tf.nn.dropout(h3, p), lambda: h3)\n",
    "        out = tf.matmul(h3, W['logits']) + b['logits']\n",
    "\n",
    "        hidden_to_bottleneck = f(tf.matmul(h2, W['bottleneck']) + b['bottleneck'])\n",
    "        d1 = f(tf.matmul(hidden_to_bottleneck, W['decode1']) + b['decode1'])\n",
    "        d1 = tf.cond(is_training, lambda: tf.nn.dropout(d1, p), lambda: d1)\n",
    "        d2 = f(tf.matmul(d1, W['decode2']) + b['decode2'])\n",
    "        d2 = tf.cond(is_training, lambda: tf.nn.dropout(d2, p), lambda: d2)\n",
    "        recreation = tf.matmul(d2, W['reconstruction']) + b['reconstruction']\n",
    "\n",
    "        risk1 = f(tf.matmul(out, W['logits_to_risk1']) +\n",
    "                  tf.matmul(tf.square(x - recreation), W['residual_to_risk1']) +\n",
    "                  tf.matmul(h2, W['hidden_to_risk1']) + b['risk1'])\n",
    "        risk2 = f(tf.matmul(risk1, W['risk2']) + b['risk2'])\n",
    "        risk_out = tf.matmul(risk2, W['risk'])\n",
    "\n",
    "        return out, recreation, tf.squeeze(risk_out)\n",
    "\n",
    "    logits, reconstruction, risk = feedforward(x)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y)) +\\\n",
    "           0.1 * tf.reduce_mean(tf.square(x - reconstruction)) +\\\n",
    "           1e-4*(tf.nn.l2_loss(W['1']) + tf.nn.l2_loss(W['2']) + tf.nn.l2_loss(W['3']) +\n",
    "                 tf.nn.l2_loss(W['bottleneck']) + tf.nn.l2_loss(W['decode1']) + tf.nn.l2_loss(W['decode2']))\n",
    "\n",
    "    lr = tf.constant(learning_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "    compute_error = tf.reduce_mean(tf.to_float(tf.not_equal(tf.argmax(logits, 1), y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Number of training examples 4767\n",
      "Number of validation examples 500\n",
      "Number of testing examples 1907\n"
     ]
    }
   ],
   "source": [
    "print('Loading Data')\n",
    "data = h5.File(\"train.h5\")\n",
    "X_train = data['X'][()]\n",
    "Y_train = data['y'][()]\n",
    "train_idxs = data['start_idx'][()]\n",
    "\n",
    "# get validation set\n",
    "X_val = X_train[-500:]\n",
    "Y_val = Y_train[-500:]\n",
    "val_idxs = train_idxs[-500:]\n",
    "X_train = X_train[:-500]\n",
    "Y_train = Y_train[:-500]\n",
    "train_idxs = train_idxs[:-500]\n",
    "\n",
    "train_mean = np.mean(X_train, axis=(0,1))\n",
    "train_std = np.std(X_train, axis=(0,1))\n",
    "X_train -= train_mean\n",
    "X_train /= (train_std + 1e-11)\n",
    "\n",
    "# NOTE: the test set is not the core test set but the entire, so the it's easier\n",
    "data = h5.File(\"test.h5\")\n",
    "X_test = data['X'][()] - train_mean\n",
    "Y_test = data['y'][()]\n",
    "test_idxs = data['start_idx'][()]\n",
    "X_test -= train_mean\n",
    "X_test /= (train_std + 1e-11)\n",
    "del data\n",
    "print('Number of training examples', X_train.shape[0])\n",
    "print('Number of validation examples', X_val.shape[0])\n",
    "print('Number of testing examples', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession(graph=graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_sample_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"in_sample\")\n",
    "out_of_sample_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, \"out_of_sample\")\n",
    "sess.run(tf.initialize_variables(set(tf.all_variables()) - set(out_of_sample_vars)))\n",
    "\n",
    "risk_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(risk, risk_labels))\n",
    "phase2_vars = list(set(tf.all_variables()) - set(in_sample_vars))\n",
    "risk_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(risk_loss, var_list=phase2_vars)\n",
    "sess.run(tf.initialize_variables(set(tf.all_variables()) - set(in_sample_vars)))\n",
    "\n",
    "compute_risk_error = tf.reduce_mean(tf.to_float(tf.not_equal(tf.to_int64(tf.round(tf.sigmoid(risk))),\n",
    "                                                             tf.to_int64(tf.round(risk_labels)))))\n",
    "\n",
    "# could collapse this into an \"initialize all\" statement but that might have less fecundity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "saver.restore(sess, \"./fcn.ckpt\")\n",
    "\n",
    "print('Model restored')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tf.nn.softmax(logits)\n",
    "s_prob = tf.reduce_max(s, reduction_indices=[1], keep_dims=True)\n",
    "kl_all = tf.log(39.) + tf.reduce_sum(s * tf.log(tf.abs(s) + 1e-11), reduction_indices=[1], keep_dims=True)\n",
    "m_all, v_all = tf.nn.moments(kl_all, axes=[0])\n",
    "\n",
    "logits_right = tf.boolean_mask(logits, tf.equal(tf.argmax(logits, 1), y))\n",
    "s_right = tf.nn.softmax(logits_right)\n",
    "s_right_prob = tf.reduce_max(s_right, reduction_indices=[1], keep_dims=True)\n",
    "kl_right = tf.log(39.) + tf.reduce_sum(s_right * tf.log(tf.abs(s_right) + 1e-11), reduction_indices=[1], keep_dims=True)\n",
    "m_right, v_right = tf.nn.moments(kl_right, axes=[0])\n",
    "\n",
    "logits_wrong = tf.boolean_mask(logits, tf.not_equal(tf.argmax(logits, 1), y))\n",
    "s_wrong = tf.nn.softmax(logits_wrong)\n",
    "s_wrong_prob = tf.reduce_max(s_wrong, reduction_indices=[1], keep_dims=True)\n",
    "kl_wrong = tf.log(39.) + tf.reduce_sum(s_wrong * tf.log(tf.abs(s_wrong) + 1e-11), reduction_indices=[1], keep_dims=True)\n",
    "m_wrong, v_wrong = tf.nn.moments(kl_wrong, axes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kl_a, kl_r, kl_w, s_p, s_rp, s_wp = [], [], [], [], [], []\n",
    "err_total = 0\n",
    "\n",
    "for i in range(X_test.shape[0]//batch_size):\n",
    "    offset = i * batch_size\n",
    "\n",
    "    _bx, mask_x, _by = X_test[offset:offset+batch_size], test_idxs[offset:offset+batch_size], Y_test[offset:offset+batch_size]\n",
    "\n",
    "    bx, by = [], []\n",
    "    for i in range(_bx.shape[0]):\n",
    "        sentence_frames = add_context(_bx[i][mask_x[i]:])\n",
    "        bx.append(sentence_frames)\n",
    "        by.append(_by[i][mask_x[i]:])\n",
    "\n",
    "    bx, by = np.concatenate(bx), np.concatenate(by)\n",
    "\n",
    "    err, kl_a_curr, kl_r_curr, kl_w_curr, s_p_curr, s_rp_curr, s_wp_curr = sess.run(\n",
    "        [100*compute_error, kl_all, kl_right, kl_wrong, s_prob, s_right_prob, s_wrong_prob],\n",
    "        feed_dict={x: bx, y: by, is_training: False})\n",
    "\n",
    "    kl_a.append(kl_a_curr)\n",
    "    kl_r.append(kl_r_curr)\n",
    "    kl_w.append(kl_w_curr)\n",
    "    s_p.append(s_p_curr)\n",
    "    s_rp.append(s_rp_curr)\n",
    "    s_wp.append(s_wp_curr)\n",
    "    err_total += err\n",
    "\n",
    "err_total /= X_test.shape[0]//batch_size\n",
    "kl_a = np.concatenate(kl_a)\n",
    "kl_r = np.concatenate(kl_r)\n",
    "kl_w = np.concatenate(kl_w)\n",
    "s_p = np.concatenate(s_p)\n",
    "s_rp = np.concatenate(s_rp)\n",
    "s_wp = np.concatenate(s_wp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):\n",
      "29.6853021201 | 0.759521 0.230741 | 0.82219 0.194918 | 0.611037 0.240867\n",
      "\n",
      "Success Detection\n",
      "Success base rate (%): 70.31\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 87.8\n",
      "AUROC (%): 75.54\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 87.99\n",
      "AUROC (%): 76.14\n",
      "\n",
      "Error Detection\n",
      "Error base rate (%): 29.69\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 54.25\n",
      "AUROC (%): 75.54\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 56.42\n",
      "AUROC (%): 76.14\n"
     ]
    }
   ],
   "source": [
    "print('Frame Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):')\n",
    "print(err_total, '|', np.mean(s_p), np.std(s_p), '|', np.mean(s_rp), np.std(s_rp), '|', np.mean(s_wp), np.std(s_wp))\n",
    "\n",
    "print('\\nSuccess Detection')\n",
    "print('Success base rate (%):', round(100-err_total,2))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "safe, risky = kl_r, kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Prediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = s_rp, s_wp\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "\n",
    "print('\\nError Detection')\n",
    "print('Error base rate (%):', round(err_total,2))\n",
    "safe, risky = -kl_r, -kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Prediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = -s_rp, -s_wp\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base rates are incorrectly printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "airport Example Prediction Probability (mean, std):\n",
      "0.678342 0.164252\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 74.67\n",
      "AUROC (%): 66.61\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 74.16\n",
      "AUROC (%): 65.34\n",
      "Normality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 77.21\n",
      "AUROC (%): 75.64\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 76.9\n",
      "AUROC (%): 74.99\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "KL[p||u]: Abnormality Detection\n",
      "AUPR (%): 57.49\n",
      "AUROC (%): 66.61\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 55.35\n",
      "AUROC (%): 65.34\n",
      "Abnormality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 74.33\n",
      "AUROC (%): 75.64\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 72.63\n",
      "AUROC (%): 74.99\n",
      "\n",
      "\n",
      "babble Example Prediction Probability (mean, std):\n",
      "0.400827 0.0942026\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 93.95\n",
      "AUROC (%): 91.39\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 92.6\n",
      "AUROC (%): 88.93\n",
      "Normality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 95.66\n",
      "AUROC (%): 95.33\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 95.02\n",
      "AUROC (%): 94.34\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "KL[p||u]: Abnormality Detection\n",
      "AUPR (%): 83.26\n",
      "AUROC (%): 91.39\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 78.29\n",
      "AUROC (%): 88.93\n",
      "Abnormality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 93.34\n",
      "AUROC (%): 95.33\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 91.35\n",
      "AUROC (%): 94.34\n",
      "\n",
      "\n",
      "car Example Prediction Probability (mean, std):\n",
      "0.710342 0.184567\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 68.1\n",
      "AUROC (%): 61.48\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 67.78\n",
      "AUROC (%): 60.65\n",
      "Normality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 69.41\n",
      "AUROC (%): 70.42\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 69.24\n",
      "AUROC (%): 70.06\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "KL[p||u]: Abnormality Detection\n",
      "AUPR (%): 54.62\n",
      "AUROC (%): 61.48\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 53.1\n",
      "AUROC (%): 60.65\n",
      "Abnormality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 71.32\n",
      "AUROC (%): 70.42\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 70.31\n",
      "AUROC (%): 70.06\n",
      "\n",
      "\n",
      "exhibition Example Prediction Probability (mean, std):\n",
      "0.445379 0.158559\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 89.95\n",
      "AUROC (%): 87.01\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 88.9\n",
      "AUROC (%): 85.05\n",
      "Normality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 92.06\n",
      "AUROC (%): 92.11\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 91.55\n",
      "AUROC (%): 91.29\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "KL[p||u]: Abnormality Detection\n",
      "AUPR (%): 81.98\n",
      "AUROC (%): 87.01\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 78.14\n",
      "AUROC (%): 85.05\n",
      "Abnormality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 92.18\n",
      "AUROC (%): 92.11\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 90.93\n",
      "AUROC (%): 91.29\n",
      "\n",
      "\n",
      "restaurant Example Prediction Probability (mean, std):\n",
      "0.736358 0.169534\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 67.92\n",
      "AUROC (%): 59.03\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 67.64\n",
      "AUROC (%): 58.27\n",
      "Normality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 69.7\n",
      "AUROC (%): 68.36\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 69.55\n",
      "AUROC (%): 68.01\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "KL[p||u]: Abnormality Detection\n",
      "AUPR (%): 51.82\n",
      "AUROC (%): 59.03\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 50.71\n",
      "AUROC (%): 58.27\n",
      "Abnormality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 68.28\n",
      "AUROC (%): 68.36\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 67.35\n",
      "AUROC (%): 68.01\n",
      "\n",
      "\n",
      "subway Example Prediction Probability (mean, std):\n",
      "0.665456 0.169276\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 74.69\n",
      "AUROC (%): 67.6\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 74.26\n",
      "AUROC (%): 66.43\n",
      "Normality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 76.99\n",
      "AUROC (%): 76.4\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 76.79\n",
      "AUROC (%): 75.87\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "KL[p||u]: Abnormality Detection\n",
      "AUPR (%): 58.64\n",
      "AUROC (%): 67.6\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 56.48\n",
      "AUROC (%): 66.43\n",
      "Abnormality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 75.4\n",
      "AUROC (%): 76.4\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 73.82\n",
      "AUROC (%): 75.87\n",
      "\n",
      "\n",
      "street Example Prediction Probability (mean, std):\n",
      "0.508353 0.120797\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 88.83\n",
      "AUROC (%): 84.26\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 87.48\n",
      "AUROC (%): 81.45\n",
      "Normality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 91.36\n",
      "AUROC (%): 90.44\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 90.63\n",
      "AUROC (%): 89.09\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "KL[p||u]: Abnormality Detection\n",
      "AUPR (%): 75.39\n",
      "AUROC (%): 84.26\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 69.17\n",
      "AUROC (%): 81.45\n",
      "Abnormality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 88.7\n",
      "AUROC (%): 90.44\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 85.49\n",
      "AUROC (%): 89.09\n",
      "\n",
      "\n",
      "train Example Prediction Probability (mean, std):\n",
      "0.632496 0.15643\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 79.3\n",
      "AUROC (%): 71.83\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 78.66\n",
      "AUROC (%): 70.32\n",
      "Normality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 82.19\n",
      "AUROC (%): 80.42\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 81.81\n",
      "AUROC (%): 79.66\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "KL[p||u]: Abnormality Detection\n",
      "AUPR (%): 61.79\n",
      "AUROC (%): 71.83\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 59.12\n",
      "AUROC (%): 70.32\n",
      "Abnormality base rate (%): 58.71\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 78.44\n",
      "AUROC (%): 80.42\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 76.64\n",
      "AUROC (%): 79.66\n"
     ]
    }
   ],
   "source": [
    "for oos_name in ['airport', 'babble', 'car', 'exhibition', 'restaurant', 'subway', 'street', 'train']:\n",
    "    \n",
    "    data = h5.File(\"test_\" + oos_name + \".h5\")     # real noise at a volume of 30%\n",
    "    oos_x = data['X'][()]\n",
    "    oos_y = data['y'][()]\n",
    "    oos_idxs = data['start_idx'][()]\n",
    "    oos_x -= train_mean\n",
    "    oos_x /= (train_std + 1e-11)\n",
    "    \n",
    "    kl_oos = []\n",
    "    s_p_oos = []\n",
    "    \n",
    "    for i in range(oos_x.shape[0]//batch_size):\n",
    "        offset = i * batch_size\n",
    "        \n",
    "        _bx, mask_x, _by = oos_x[offset:offset+batch_size], oos_idxs[offset:offset+batch_size], oos_y[offset:offset+batch_size]\n",
    "\n",
    "        bx, by = [], []\n",
    "        for i in range(_bx.shape[0]):\n",
    "            sentence_frames = add_context(_bx[i][mask_x[i]:])\n",
    "            bx.append(sentence_frames)\n",
    "            by.append(_by[i][mask_x[i]:])\n",
    "\n",
    "        bx, by = np.concatenate(bx), np.concatenate(by)\n",
    "        \n",
    "        kl_oos_curr, s_p_oos_curr = sess.run([kl_all, s_prob], feed_dict={x: bx, is_training: False})\n",
    "\n",
    "        kl_oos.append(kl_oos_curr)\n",
    "        s_p_oos.append(s_p_oos_curr)\n",
    "\n",
    "    print('\\n\\n' + oos_name, 'Example Prediction Probability (mean, std):')\n",
    "    print(np.mean(np.concatenate(s_p_oos)), np.std(np.concatenate(s_p_oos)))\n",
    "\n",
    "    print('\\nNormality Detection')\n",
    "    print('Normality base rate (%):', round(50,2))\n",
    "    print('KL[p||u]: Normality Detection')\n",
    "    safe, risky = kl_a, np.concatenate(kl_oos)\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection')\n",
    "    safe, risky = s_p, np.concatenate(s_p_oos)\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Normality base rate (%):', round(100*1./(1 + 1 - err_total/100),2))\n",
    "    print('KL[p||u]: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = kl_r, np.concatenate(kl_oos)\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = s_rp, np.concatenate(s_p_oos)\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "\n",
    "    print('\\nAbnormality Detection')\n",
    "    print('Abnormality base rate (%):', round(50,2))\n",
    "    print('KL[p||u]: Abnormality Detection')\n",
    "    safe, risky = -kl_a, -np.concatenate(kl_oos)\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection')\n",
    "    safe, risky = -s_p, -np.concatenate(s_p_oos)\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Abnormality base rate (%):', round(100*1./(1 + 1 - err_total/100),2))\n",
    "    print('KL[p||u]: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = -kl_r, -np.concatenate(kl_oos)\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = -s_rp, -np.concatenate(s_p_oos)\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auxiliary Decoder and the Abnormality Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading OOD Data\n"
     ]
    }
   ],
   "source": [
    "print('Loading OOD Data')\n",
    "data = h5.File(\"train_p_0.02.h5\")\n",
    "p_02 = data['X'][()]\n",
    "p_02_idxs = data['start_idx'][()]\n",
    "p_02 -= train_mean\n",
    "p_02 /= (train_std + 1e-11)\n",
    "\n",
    "data = h5.File(\"train_w_0.005.h5\")\n",
    "w_005 = data['X'][()]\n",
    "w_005_idxs = data['start_idx'][()]\n",
    "w_005 -= train_mean\n",
    "w_005 /= (train_std + 1e-11)\n",
    "\n",
    "data = h5.File(\"train_b_0.05.h5\")\n",
    "b_05 = data['X'][()]\n",
    "b_05_idxs = data['start_idx'][()]\n",
    "b_05 -= train_mean\n",
    "b_05 /= (train_std + 1e-11)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the risk neuron\n",
      "Epoch: 0 | ema of risk for epoch: 5.25729855974 error (%): 2.31989826724\n",
      "Epoch: 1 | ema of risk for epoch: 0.0377295030432 error (%): 1.15339131546\n"
     ]
    }
   ],
   "source": [
    "print('Training the risk neuron')\n",
    "\n",
    "num_batches = X_train.shape[0] // batch_size\n",
    "err_ema = 1./n_labels\n",
    "risk_loss_ema = 0.3  # -log(0.5)\n",
    "\n",
    "for epoch in range(2):  # 3 epoch\n",
    "    # shuffle data\n",
    "    indices = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_train = X_train[indices]\n",
    "    Y_train = Y_train[indices]\n",
    "    train_idxs = train_idxs[indices]\n",
    "    \n",
    "    p_02 = p_02[indices]\n",
    "    p_02_idxs = p_02_idxs[indices]\n",
    "    w_005 = w_005[indices]\n",
    "    w_005_idxs = w_005_idxs[indices]\n",
    "    b_05 = b_05[indices]\n",
    "    b_05_idxs = b_05_idxs[indices]\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        offset = i * batch_size\n",
    "        \n",
    "        # get in-sample data\n",
    "        _bx1, mask_x1 = X_train[offset:offset+batch_size//4], train_idxs[offset:offset+batch_size//4]\n",
    "        bx1 = []\n",
    "        for i in range(_bx1.shape[0]):\n",
    "            sentence_frames = add_context(_bx1[i][mask_x1[i]:])\n",
    "            bx1.append(sentence_frames)\n",
    "        bx1 = np.concatenate(bx1)\n",
    "\n",
    "        val_indices = np.arange(X_val.shape[0])\n",
    "        np.random.shuffle(val_indices)\n",
    "        _bx2, mask_x2 = X_val[val_indices[0:batch_size//4]], val_idxs[val_indices[0:batch_size//4]]\n",
    "\n",
    "        bx2 = []\n",
    "        for i in range(_bx2.shape[0]):\n",
    "            sentence_frames = add_context(_bx2[i][mask_x2[i]:])\n",
    "            bx2.append(sentence_frames)\n",
    "        bx2 = np.concatenate(bx2)\n",
    "        \n",
    "        # get oos data\n",
    "        \n",
    "        _bx3, mask_x3 = p_02[offset:offset+batch_size//6], p_02_idxs[offset:offset+batch_size//6]\n",
    "        bx3 = []\n",
    "        for i in range(_bx3.shape[0]):\n",
    "            sentence_frames = add_context(_bx3[i][mask_x3[i]:])\n",
    "            bx3.append(sentence_frames)\n",
    "        bx3 = np.concatenate(bx3)\n",
    "        \n",
    "        _bx4, mask_x4 = w_005[offset:offset+batch_size//6], w_005_idxs[offset:offset+batch_size//6]\n",
    "        bx4 = []\n",
    "        for i in range(_bx4.shape[0]):\n",
    "            sentence_frames = add_context(_bx4[i][mask_x4[i]:])\n",
    "            bx4.append(sentence_frames)\n",
    "        bx4 = np.concatenate(bx4)\n",
    "        \n",
    "        _bx5, mask_x5 = b_05[offset:offset+batch_size//6], b_05_idxs[offset:offset+batch_size//6]\n",
    "        bx5 = []\n",
    "        for i in range(_bx5.shape[0]):\n",
    "            sentence_frames = add_context(_bx5[i][mask_x5[i]:])\n",
    "            bx5.append(sentence_frames)\n",
    "        bx5 = np.concatenate(bx5)\n",
    "\n",
    "        risks = np.zeros(bx1.shape[0] + bx2.shape[0] + bx3.shape[0] + bx4.shape[0] + bx5.shape[0])\n",
    "        risks[:bx1.shape[0] + bx2.shape[0]] = 1\n",
    "        bx = np.concatenate((bx1, bx2, bx3, bx4, bx5), axis=0)\n",
    "\n",
    "        _, rl, err = sess.run([risk_optimizer, risk_loss, compute_risk_error],\n",
    "                              feed_dict={x: bx, risk_labels: risks, is_training: False})\n",
    "        risk_loss_ema = risk_loss_ema * 0.95 + 0.05 * rl\n",
    "        err_ema = err_ema * 0.95 + 0.05 * err\n",
    "\n",
    "    print('Epoch:', epoch, '|', 'ema of risk for epoch:', risk_loss_ema, 'error (%):', 100*err_ema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "err_total = 0\n",
    "risk_err_total = 0\n",
    "risk_total = []\n",
    "risk_right_total = []\n",
    "risk_wrong_total = []\n",
    "conf_total = []\n",
    "\n",
    "for i in range(X_test.shape[0]//batch_size):\n",
    "    offset = i * batch_size\n",
    "    _bx, mask_x, _by = X_test[offset:offset+batch_size], test_idxs[offset:offset+batch_size], Y_test[offset:offset+batch_size]\n",
    "\n",
    "    bx, by = [], []\n",
    "    for i in range(_bx.shape[0]):\n",
    "        sentence_frames = add_context(_bx[i][mask_x[i]:])\n",
    "        bx.append(sentence_frames)\n",
    "        by.append(_by[i][mask_x[i]:])\n",
    "\n",
    "    bx, by = np.concatenate(bx), np.concatenate(by)\n",
    "\n",
    "    err, r_err, r, conf = sess.run([100*compute_error, 100*compute_risk_error,\n",
    "                                    tf.sigmoid(risk), tf.nn.softmax(logits)],\n",
    "                                   feed_dict={x: bx, y: by, risk_labels: np.ones(by.shape[0]), is_training: False})\n",
    "\n",
    "    r_right = r[np.argmax(conf, axis=1).astype(np.int32) == by]\n",
    "    r_wrong = r[np.argmax(conf, axis=1).astype(np.int32) != by]\n",
    "\n",
    "    err_total += err\n",
    "    risk_err_total += r_err\n",
    "    risk_total.append(r)\n",
    "    conf_total.append(conf)\n",
    "    risk_right_total.append(r_right)\n",
    "    risk_wrong_total.append(r_wrong)\n",
    "\n",
    "risk_err_total /= X_test.shape[0]//batch_size\n",
    "err_total /= X_test.shape[0]//batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIMIT Clean Frame Error (%) | TIMIT Frame Riskiness Error (0.5 cutoff) (%) | Frame Confidence (mean, std):\n",
      "29.6853021201 | 2.56435128383 | 0.759521 0.230741\n"
     ]
    }
   ],
   "source": [
    "print('TIMIT Clean Frame Error (%) | TIMIT Frame Riskiness Error (0.5 cutoff) (%) | Frame Confidence (mean, std):')\n",
    "print(err_total, '|', risk_err_total, '|', np.mean(np.max(np.concatenate(conf_total), axis=1)),\n",
    "      np.std(np.max(np.concatenate(conf_total), axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Risk Neuron: Clean Right/Wrong classification distinction\n",
      "AUPR 0.841913118528\n",
      "AUROC 0.635562324801\n"
     ]
    }
   ],
   "source": [
    "safe, risky = np.concatenate(risk_right_total).reshape(-1,1), np.concatenate(risk_wrong_total).reshape(-1,1)\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "\n",
    "print('Risk Neuron: Clean Right/Wrong classification distinction')\n",
    "print('AUPR', sk.average_precision_score(labels, examples))\n",
    "print('AUROC', sk.roc_auc_score(labels, examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implication is that the logistic regression neuron is as not great for detecting whether the example is misclassified. Perhaps is incorrect examples were treated as negative examples we would do better.\n",
    "\n",
    "Now let's try OOD examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update: Base rates should be as above. err_total is updated while it should refer to an older value. They're incorrectly printed but this doesn't affect anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TIMIT airport Frame Error (%) | TIMIT Frame Riskiness Error (0.5 cutoff) (%) | Frame Confidence (mean, std):\n",
      "30.3071249946 | 2.62155895213 | 0.678342 0.164252\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "Normality Detection\n",
      "AUPR (%): 99.66\n",
      "AUROC (%): 99.57\n",
      "Normality base rate (%): 58.93\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 99.62\n",
      "AUROC (%): 99.65\n",
      "\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "Abnormality Detection\n",
      "AUPR (%): 99.45\n",
      "AUROC (%): 99.57\n",
      "Abnormality base rate (%): 58.93\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 99.68\n",
      "AUROC (%): 99.65\n",
      "\n",
      "TIMIT babble Frame Error (%) | TIMIT Frame Riskiness Error (0.5 cutoff) (%) | Frame Confidence (mean, std):\n",
      "30.308092085 | 0.00997679636388 | 0.400827 0.0942026\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "Normality Detection\n",
      "AUPR (%): 99.94\n",
      "AUROC (%): 99.91\n",
      "Normality base rate (%): 58.93\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 99.93\n",
      "AUROC (%): 99.92\n",
      "\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "Abnormality Detection\n",
      "AUPR (%): 99.81\n",
      "AUROC (%): 99.91\n",
      "Abnormality base rate (%): 58.93\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 99.89\n",
      "AUROC (%): 99.92\n",
      "\n",
      "TIMIT car Frame Error (%) | TIMIT Frame Riskiness Error (0.5 cutoff) (%) | Frame Confidence (mean, std):\n",
      "30.3905919285 | 23.7249792552 | 0.710342 0.184567\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "Normality Detection\n",
      "AUPR (%): 98.49\n",
      "AUROC (%): 98.01\n",
      "Normality base rate (%): 58.96\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 98.44\n",
      "AUROC (%): 98.39\n",
      "\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "Abnormality Detection\n",
      "AUPR (%): 97.13\n",
      "AUROC (%): 98.01\n",
      "Abnormality base rate (%): 58.96\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 98.31\n",
      "AUROC (%): 98.39\n",
      "\n",
      "TIMIT exhibition Frame Error (%) | TIMIT Frame Riskiness Error (0.5 cutoff) (%) | Frame Confidence (mean, std):\n",
      "31.1150914855 | 32.5871666407 | 0.445379 0.158559\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "Normality Detection\n",
      "AUPR (%): 97.71\n",
      "AUROC (%): 97.13\n",
      "Normality base rate (%): 59.21\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 97.65\n",
      "AUROC (%): 97.7\n",
      "\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "Abnormality Detection\n",
      "AUPR (%): 96.37\n",
      "AUROC (%): 97.13\n",
      "Abnormality base rate (%): 59.21\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 97.87\n",
      "AUROC (%): 97.7\n",
      "\n",
      "TIMIT restaurant Frame Error (%) | TIMIT Frame Riskiness Error (0.5 cutoff) (%) | Frame Confidence (mean, std):\n",
      "30.3446945902 | 66.1732135708 | 0.736358 0.169534\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "Normality Detection\n",
      "AUPR (%): 95.63\n",
      "AUROC (%): 94.07\n",
      "Normality base rate (%): 58.94\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 95.63\n",
      "AUROC (%): 95.27\n",
      "\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "Abnormality Detection\n",
      "AUPR (%): 91.2\n",
      "AUROC (%): 94.07\n",
      "Abnormality base rate (%): 58.94\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 94.79\n",
      "AUROC (%): 95.27\n",
      "\n",
      "TIMIT subway Frame Error (%) | TIMIT Frame Riskiness Error (0.5 cutoff) (%) | Frame Confidence (mean, std):\n",
      "30.3169423443 | 52.0356083886 | 0.665456 0.169276\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "Normality Detection\n",
      "AUPR (%): 96.05\n",
      "AUROC (%): 95.01\n",
      "Normality base rate (%): 58.93\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 95.96\n",
      "AUROC (%): 95.99\n",
      "\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "Abnormality Detection\n",
      "AUPR (%): 93.37\n",
      "AUROC (%): 95.01\n",
      "Abnormality base rate (%): 58.93\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 96.07\n",
      "AUROC (%): 95.99\n",
      "\n",
      "TIMIT street Frame Error (%) | TIMIT Frame Riskiness Error (0.5 cutoff) (%) | Frame Confidence (mean, std):\n",
      "30.3095885131 | 30.8250142437 | 0.508353 0.120797\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "Normality Detection\n",
      "AUPR (%): 98.57\n",
      "AUROC (%): 97.97\n",
      "Normality base rate (%): 58.93\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 98.54\n",
      "AUROC (%): 98.36\n",
      "\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "Abnormality Detection\n",
      "AUPR (%): 96.89\n",
      "AUROC (%): 97.97\n",
      "Abnormality base rate (%): 58.93\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 98.18\n",
      "AUROC (%): 98.36\n",
      "\n",
      "TIMIT train Frame Error (%) | TIMIT Frame Riskiness Error (0.5 cutoff) (%) | Frame Confidence (mean, std):\n",
      "30.3597302518 | 0.0548225726648 | 0.632496 0.15643\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 50\n",
      "Normality Detection\n",
      "AUPR (%): 99.96\n",
      "AUROC (%): 99.96\n",
      "Normality base rate (%): 58.95\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 99.96\n",
      "AUROC (%): 99.96\n",
      "\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 50\n",
      "Abnormality Detection\n",
      "AUPR (%): 99.96\n",
      "AUROC (%): 99.96\n",
      "Abnormality base rate (%): 58.95\n",
      "Normality Detection (relative to correct examples)\n",
      "AUPR (%): 99.97\n",
      "AUROC (%): 99.96\n"
     ]
    }
   ],
   "source": [
    "for oos_name in ['airport', 'babble', 'car', 'exhibition', 'restaurant', 'subway', 'street', 'train']:\n",
    "    \n",
    "    data = h5.File(\"test_\" + oos_name + \".h5\")     # real noise at a volume of 30%\n",
    "    oos_x = data['X'][()]\n",
    "    oos_y = data['y'][()]\n",
    "    oos_idxs = data['start_idx'][()]\n",
    "    oos_x -= train_mean\n",
    "    oos_x /= (train_std + 1e-11)\n",
    "    \n",
    "    err_total = 0\n",
    "    risk_err_total = 0\n",
    "    risk_total_oos = []\n",
    "    risk_right_total_oos = []\n",
    "    risk_wrong_total_oos = []\n",
    "    conf_total_oos = []\n",
    "    \n",
    "    for i in range(oos_x.shape[0]//batch_size):\n",
    "        offset = i * batch_size\n",
    "        \n",
    "        _bx, mask_x, _by = oos_x[offset:offset+batch_size], oos_idxs[offset:offset+batch_size], oos_y[offset:offset+batch_size]\n",
    "\n",
    "        bx, by = [], []\n",
    "        for i in range(_bx.shape[0]):\n",
    "            sentence_frames = add_context(_bx[i][mask_x[i]:])\n",
    "            bx.append(sentence_frames)\n",
    "            by.append(_by[i][mask_x[i]:])\n",
    "\n",
    "        bx, by = np.concatenate(bx), np.concatenate(by)\n",
    "\n",
    "        \n",
    "        err, r_err, r, conf = sess.run([100*compute_error, 100*compute_risk_error,\n",
    "                                        tf.sigmoid(risk), tf.nn.softmax(logits)],\n",
    "                                       feed_dict={x: bx, y: by, risk_labels: np.zeros(by.shape[0]), is_training: False})\n",
    "\n",
    "        r_right = r[np.argmax(conf, axis=1).astype(np.int32) == by]\n",
    "        r_wrong = r[np.argmax(conf, axis=1).astype(np.int32) != by]\n",
    "\n",
    "        err_total += err\n",
    "        risk_err_total += r_err\n",
    "        risk_total_oos.append(r)\n",
    "        conf_total_oos.append(conf)\n",
    "        risk_right_total_oos.append(r_right)\n",
    "        risk_wrong_total_oos.append(r_wrong)\n",
    "\n",
    "\n",
    "    risk_err_total /= oos_x.shape[0]//batch_size\n",
    "    err_total /= oos_x.shape[0]//batch_size\n",
    "    \n",
    "    print('\\nTIMIT', oos_name, 'Frame Error (%) | TIMIT Frame Riskiness Error (0.5 cutoff) (%) | Frame Confidence (mean, std):')\n",
    "    print(err_total, '|', risk_err_total, '|', np.mean(np.max(np.concatenate(conf_total_oos), axis=1)),\n",
    "      np.std(np.max(np.concatenate(conf_total_oos), axis=1)))\n",
    "    \n",
    "    risk_total = np.concatenate(risk_total).reshape(-1,1)\n",
    "    risk_right_total = np.concatenate(risk_right_total).reshape(-1,1)\n",
    "    risk_total_oos = np.concatenate(risk_total_oos).reshape(-1,1)\n",
    "\n",
    "    print('\\nNormality Detection')\n",
    "    print('Normality base rate (%):', round(50,2))\n",
    "    print('Normality Detection')\n",
    "    safe, risky = risk_total, risk_total_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Normality base rate (%):', round(100*1./(1 + 1 - err_total/100),2))\n",
    "    print('Normality Detection (relative to correct examples)')\n",
    "    safe, risky = risk_right_total, risk_total_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('\\n\\nAbnormality Detection')\n",
    "    print('Abnormality base rate (%):', round(50,2))\n",
    "    print('Abnormality Detection')\n",
    "    safe, risky = 1 - risk_total, 1 - risk_total_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Abnormality base rate (%):', round(100*1./(1 + 1 - err_total/100),2))\n",
    "    print('Abnormality Detection (relative to correct examples)')\n",
    "    safe, risky = 1 - risk_right_total, 1 - risk_total_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
