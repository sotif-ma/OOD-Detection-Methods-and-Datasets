{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename='./data/r52-train.txt'):\n",
    "    '''\n",
    "    :param filename: the system location of the data to load\n",
    "    :return: the text (x) and its label (y)\n",
    "             the text is a list of words and is not processed\n",
    "    '''\n",
    "\n",
    "    topic_to_num = {\"acq\": 0,\"alum\": 1,\"bop\": 2,\"carcass\": 3,\"cocoa\": 4,\"coffee\": 5,\"copper\": 6,\n",
    "                    \"cotton\": 7,\"cpi\": 8,\"cpu\": 9,\"crude\": 10,\"dlr\": 11,\"earn\": 12,\"fuel\": 13,\"gas\": 14,\n",
    "                    \"gnp\": 15,\"gold\": 16,\"grain\": 17,\"heat\": 18,\"housing\": 19,\"income\": 20,\"instal-debt\": 21,\n",
    "                    \"interest\": 22,\"ipi\": 23,\"iron-steel\": 24,\"jet\": 25,\"jobs\": 26,\"lead\": 27,\"lei\": 28,\n",
    "                    \"livestock\": 29,\"lumber\": 30,\"meal-feed\": 31,\"money-fx\": 32,\"money-supply\": 33,\n",
    "                    \"nat-gas\": 34,\"nickel\": 35,\"orange\": 36,\"pet-chem\": 37,\"platinum\": 38,\"potato\": 39,\n",
    "                    \"reserves\": 40,\"retail\": 41,\"rubber\": 42,\"ship\": 43,\"strategic-metal\": 44,\"sugar\": 45,\n",
    "                    \"tea\": 46,\"tin\": 47,\"trade\": 48,\"veg-oil\": 49,\"wpi\": 50,\"zinc\": 51}\n",
    "    \n",
    "    # stop words taken from nltk\n",
    "    stop_words = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours',\n",
    "                  'yourself','yourselves','he','him','his','himself','she','her','hers','herself',\n",
    "                  'it','its','itself','they','them','their','theirs','themselves','what','which',\n",
    "                  'who','whom','this','that','these','those','am','is','are','was','were','be',\n",
    "                  'been','being','have','has','had','having','do','does','did','doing','a','an',\n",
    "                  'the','and','but','if','or','because','as','until','while','of','at','by','for',\n",
    "                  'with','about','against','between','into','through','during','before','after',\n",
    "                  'above','below','to','from','up','down','in','out','on','off','over','under',\n",
    "                  'again','further','then','once','here','there','when','where','why','how','all',\n",
    "                  'any','both','each','few','more','most','other','some','such','no','nor','not',\n",
    "                  'only','own','same','so','than','too','very','s','t','can','will','just','don',\n",
    "                  'should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn',\n",
    "                  'doesn','hadn','hasn','haven','isn','ma','mightn','mustn','needn','shan',\n",
    "                  'shouldn','wasn','weren','won','wouldn']\n",
    "\n",
    "    x, y = [], []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = re.sub(r'\\s+', ' ', line).strip()\n",
    "            current_story = ' '.join(word for word in line.split(' ')[1:] if word not in stop_words)\n",
    "            x.append(current_story)\n",
    "            y.append(topic_to_num[line.split(' ')[0]])\n",
    "    return x, np.array(y, dtype=int)\n",
    "\n",
    "def get_vocab(dataset):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "\n",
    "    :return: a _ordered_ dictionary from words to counts\n",
    "    '''\n",
    "    vocab = {}\n",
    "\n",
    "    # create a counter for each word\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] += 1\n",
    "    \n",
    "    # sort from greatest to least by count\n",
    "    return collections.OrderedDict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def text_to_rank(dataset, _vocab, desired_vocab_size=1000):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "    :vocab: a _ordered_ dictionary of vocab words and counts from get_vocab\n",
    "    :param desired_vocab_size: the desired vocabulary size\n",
    "    words no longer in vocab become UUUNNNKKK\n",
    "    :return: the text corpus with words mapped to their vocab rank,\n",
    "    with all sufficiently infrequent words mapped to UUUNNNKKK; UUUNNNKKK has rank desired_vocab_size\n",
    "    (the infrequent word cutoff is determined by desired_vocab size)\n",
    "    '''\n",
    "    _dataset = dataset[:]     # aliasing safeguard\n",
    "    vocab_ordered = list(_vocab)\n",
    "    count_cutoff = _vocab[vocab_ordered[desired_vocab_size-2]] # get word by its rank and map to its count\n",
    "    \n",
    "    word_to_rank = {}\n",
    "    for i in range(len(vocab_ordered)):\n",
    "        # we add one to make room for any future padding symbol with value 0\n",
    "        word_to_rank[vocab_ordered[i]] = i\n",
    "    \n",
    "    for i in range(len(_dataset)):\n",
    "        example = _dataset[i]\n",
    "        example_as_list = example.split()\n",
    "        for j in range(len(example_as_list)):\n",
    "            try:\n",
    "                if _vocab[example_as_list[j]] >= count_cutoff and word_to_rank[example_as_list[j]] < desired_vocab_size:\n",
    "                    # we need to ensure that other words below the word on the edge of our desired_vocab size\n",
    "                    # are not also on the count cutoff\n",
    "                    example_as_list[j] = word_to_rank[example_as_list[j]] \n",
    "                else:\n",
    "                    example_as_list[j] = desired_vocab_size-1  # UUUNNNKKK\n",
    "            except:\n",
    "                example_as_list[j] = desired_vocab_size-1  # UUUNNNKKK\n",
    "        _dataset[i] = example_as_list\n",
    "\n",
    "    return _dataset\n",
    "\n",
    "def text_to_matrix(dataset, _vocab, desired_vocab_size=1000):\n",
    "    sequences = text_to_rank(dataset, _vocab, desired_vocab_size)\n",
    "    \n",
    "    mat = np.zeros((len(sequences), desired_vocab_size), dtype=int)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        for token in seq:\n",
    "            mat[i][token] = 1\n",
    "    \n",
    "    return mat\n",
    "\n",
    "def get_vocab(dataset):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "\n",
    "    :return: a _ordered_ dictionary from words to counts\n",
    "    '''\n",
    "    vocab = {}\n",
    "\n",
    "    # create a counter for each word\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] += 1\n",
    "\n",
    "    # sort from greatest to least by count\n",
    "    return collections.OrderedDict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def partion_data_in_two(dataset, dataset_labels, in_sample_labels, oos_labels):\n",
    "    '''\n",
    "    :param dataset: the text from text_to_rank\n",
    "    :param dataset_labels: dataset labels\n",
    "    :param in_sample_labels: a list of newsgroups which the network will/did train on\n",
    "    :param oos_labels: the complement of in_sample_labels; these newsgroups the network has never seen\n",
    "    :return: the dataset partitioned into in_sample_examples, in_sample_labels,\n",
    "    oos_examples, and oos_labels in that order\n",
    "    '''\n",
    "    _dataset = dataset[:]     # aliasing safeguard\n",
    "    _dataset_labels = dataset_labels\n",
    "    \n",
    "    in_sample_idxs = np.zeros(np.shape(_dataset_labels), dtype=bool)\n",
    "    ones_vec = np.ones(np.shape(_dataset_labels), dtype=int)\n",
    "    for label in in_sample_labels:\n",
    "        in_sample_idxs = np.logical_or(in_sample_idxs, _dataset_labels == label * ones_vec)\n",
    "\n",
    "    \n",
    "    return _dataset[in_sample_idxs], _dataset_labels[in_sample_idxs],\\\n",
    "        _dataset[np.logical_not(in_sample_idxs)], _dataset_labels[np.logical_not(in_sample_idxs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# our network trains only on a subset of classes, say 6, but class number 7 might still\n",
    "# be an in-sample label: we need to squish the labels to be in {0,...,5}\n",
    "def relabel_in_sample_labels(labels):\n",
    "    labels_as_list = labels.tolist()\n",
    "    \n",
    "    set_of_labels = []\n",
    "    for label in labels_as_list:\n",
    "        set_of_labels.append(label)\n",
    "    labels_ordered = sorted(list(set(set_of_labels)))\n",
    "    \n",
    "    relabeled = np.zeros(labels.shape, dtype=int)\n",
    "    for i in range(len(labels_as_list)):\n",
    "        relabeled[i] = labels_ordered.index(labels_as_list[i])\n",
    "    \n",
    "    return relabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "vocab_size = 1000\n",
    "num_epochs = 5\n",
    "n_hidden = 512\n",
    "nclasses_to_exclude = 12  # 0-22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_classes = np.arange(52)\n",
    "np.random.shuffle(random_classes)\n",
    "to_include = list(random_classes[:52-nclasses_to_exclude])\n",
    "to_exclude = list(random_classes[52-nclasses_to_exclude:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "print('Loading Data')\n",
    "X_train, Y_train = load_data()\n",
    "X_test, Y_test = load_data('./data/r52-test.txt')\n",
    "\n",
    "vocab = get_vocab(X_train)\n",
    "X_train = text_to_matrix(X_train, vocab, vocab_size)\n",
    "X_test = text_to_matrix(X_test, vocab, vocab_size)\n",
    "\n",
    "# shuffle\n",
    "indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_train = X_train[indices]\n",
    "Y_train = Y_train[indices]\n",
    "\n",
    "indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_test = X_test[indices]\n",
    "Y_test = Y_test[indices]\n",
    "\n",
    "in_sample_examples, in_sample_labels, oos_examples, oos_labels =\\\n",
    "partion_data_in_two(X_train, Y_train, to_include, to_exclude)\n",
    "test_in_sample_examples, test_in_sample_labels, test_oos_examples, dev_oos_labels =\\\n",
    "partion_data_in_two(X_test, Y_test, to_include, to_exclude)\n",
    "\n",
    "# safely assumes there is an example for each in_sample class in both the training and dev class \n",
    "in_sample_labels = relabel_in_sample_labels(in_sample_labels)\n",
    "test_in_sample_labels = relabel_in_sample_labels(test_in_sample_labels)\n",
    "\n",
    "num_examples = in_sample_labels.shape[0]\n",
    "num_batches = num_examples//batch_size\n",
    "\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, vocab_size])\n",
    "    y = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    # add one to vocab size for the padding symbol\n",
    "\n",
    "    W_h = tf.Variable(tf.nn.l2_normalize(tf.random_normal([vocab_size, n_hidden]), 0)/tf.sqrt(1 + 0.45))\n",
    "    b_h = tf.Variable(tf.zeros([n_hidden]))\n",
    "    \n",
    "    def gelu_fast(_x):\n",
    "        return 0.5 * _x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (_x + 0.044715 * tf.pow(_x, 3))))\n",
    "    \n",
    "    h = tf.cond(is_training,\n",
    "                lambda: tf.nn.dropout(gelu_fast(tf.matmul(x, W_h) + b_h), 0.5),\n",
    "                lambda: gelu_fast(tf.matmul(x, W_h) + b_h))\n",
    "    \n",
    "    W_out = tf.Variable(tf.nn.l2_normalize(tf.random_normal([n_hidden, 52-nclasses_to_exclude]), 0)/tf.sqrt(0.45 + 1))\n",
    "    b_out = tf.Variable(tf.zeros([52-nclasses_to_exclude]))\n",
    "    \n",
    "    logits = tf.matmul(h, W_out) + b_out\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    lr = tf.train.exponential_decay(1e-3, global_step, 4*num_batches, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss, global_step=global_step)\n",
    "\n",
    "    acc = 100*tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(logits, 1), y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "tf.initialize_all_variables().run()\n",
    "# create saver to train model\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "print('Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Minibatch loss 0.526 | Minibatch accuracy 87.500\n",
      "Epoch 2 | Minibatch loss 0.260 | Minibatch accuracy 93.750\n",
      "Epoch 3 | Minibatch loss 0.106 | Minibatch accuracy 100.000\n",
      "Epoch 4 | Minibatch loss 0.019 | Minibatch accuracy 100.000\n",
      "Epoch 5 | Minibatch loss 0.052 | Minibatch accuracy 100.000\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    # shuffle data every epoch\n",
    "    indices = np.arange(num_examples)\n",
    "    np.random.shuffle(indices)\n",
    "    in_sample_examples = in_sample_examples[indices]\n",
    "    in_sample_labels = in_sample_labels[indices]\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        offset = i * batch_size\n",
    "\n",
    "        x_batch = in_sample_examples[offset:offset + batch_size]\n",
    "        y_batch = in_sample_labels[offset:offset + batch_size]\n",
    "\n",
    "        _, l, batch_acc = sess.run([optimizer, loss, acc], feed_dict={x: x_batch, y: y_batch, is_training: True})\n",
    "\n",
    "    print('Epoch %d | Minibatch loss %.3f | Minibatch accuracy %.3f' %\n",
    "          (epoch+1, l, batch_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tf.nn.softmax(logits)\n",
    "s_prob = tf.reduce_max(s, reduction_indices=[1], keep_dims=True)\n",
    "kl_all = tf.log(52. - nclasses_to_exclude)\\\n",
    "        + tf.reduce_sum(s * tf.log(tf.abs(s) + 1e-11), reduction_indices=[1], keep_dims=True)\n",
    "m_all, v_all = tf.nn.moments(kl_all, axes=[0])\n",
    "\n",
    "logits_right = tf.boolean_mask(logits, tf.equal(tf.argmax(logits, 1), y))\n",
    "s_right = tf.nn.softmax(logits_right)\n",
    "s_right_prob = tf.reduce_max(s_right, reduction_indices=[1], keep_dims=True)\n",
    "kl_right = tf.log(52. - nclasses_to_exclude)\\\n",
    "         + tf.reduce_sum(s_right * tf.log(tf.abs(s_right) + 1e-11), reduction_indices=[1], keep_dims=True)\n",
    "m_right, v_right = tf.nn.moments(kl_right, axes=[0])\n",
    "\n",
    "logits_wrong = tf.boolean_mask(logits, tf.not_equal(tf.argmax(logits, 1), y))\n",
    "s_wrong = tf.nn.softmax(logits_wrong)\n",
    "s_wrong_prob = tf.reduce_max(s_wrong, reduction_indices=[1], keep_dims=True)\n",
    "kl_wrong = tf.log(52. - nclasses_to_exclude)\\\n",
    "           + tf.reduce_sum(s_wrong * tf.log(tf.abs(s_wrong) + 1e-11), reduction_indices=[1], keep_dims=True)\n",
    "m_wrong, v_wrong = tf.nn.moments(kl_wrong, axes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reuters52 (w/class subset) Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):\n",
      "7.93185 | 0.908305 0.205085 | 0.940961 0.157806 | 0.529264 0.291815\n",
      "\n",
      "Success Detection\n",
      "Success base rate (%): 92.07\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 99.18\n",
      "AUROC (%): 91.47\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 99.28\n",
      "AUROC (%): 92.53\n",
      "\n",
      "Error Detection\n",
      "Error base rate (%): 7.93\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 45.17\n",
      "AUROC (%): 91.47\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 51.85\n",
      "AUROC (%): 92.53\n"
     ]
    }
   ],
   "source": [
    "err, kl_a, kl_r, kl_w, s_p, s_rp, s_wp = sess.run(\n",
    "    [100 - acc, kl_all, kl_right, kl_wrong, s_prob, s_right_prob, s_wrong_prob],\n",
    "    feed_dict={x: test_in_sample_examples, y: test_in_sample_labels, is_training: False})\n",
    "\n",
    "print('Reuters52 (w/class subset) Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):')\n",
    "print(err, '|', np.mean(s_p), np.std(s_p), '|', np.mean(s_rp), np.std(s_rp), '|', np.mean(s_wp), np.std(s_wp))\n",
    "\n",
    "print('\\nSuccess Detection')\n",
    "print('Success base rate (%):', round(100-err,2))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "safe, risky = kl_r, kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Prediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = s_rp, s_wp\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "\n",
    "print('\\nError Detection')\n",
    "print('Error base rate (%):', round(err,2))\n",
    "safe, risky = -kl_r, -kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Prediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = -s_rp, -s_wp\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_ood_detection_results(error_rate_for_in, in_examples, out_examples):\n",
    "    kl_oos, s_p_oos = sess.run([kl_all, s_prob], feed_dict={x: out_examples, is_training: False})\n",
    "\n",
    "    print('OOD Example Prediction Probability (mean, std):')\n",
    "    print(np.mean(s_p_oos), np.std(s_p_oos))\n",
    "\n",
    "    print('\\nNormality Detection')\n",
    "    print('Normality base rate (%):', round(100*in_examples.shape[0]/(\n",
    "                out_examples.shape[0] + in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Normality Detection')\n",
    "    safe, risky = kl_a, kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection')\n",
    "    safe, risky = s_p, s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Normality base rate (%):', round(100*(1 - err/100)*in_examples.shape[0]/\n",
    "          (out_examples.shape[0] + (1 - err/100)*in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = kl_r, kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = s_rp, s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "\n",
    "    print('\\n\\nAbnormality Detection')\n",
    "    print('Abnormality base rate (%):', round(100*out_examples.shape[0]/(\n",
    "                out_examples.shape[0] + in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Abnormality Detection')\n",
    "    safe, risky = -kl_a, -kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection')\n",
    "    safe, risky = -s_p, -s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Abnormality base rate (%):', round(100*out_examples.shape[0]/\n",
    "          (out_examples.shape[0] + (1 - err/100)*in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = -kl_r, -kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = -s_rp, -s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Held-out subjects\n",
      "\n",
      "OOD Example Prediction Probability (mean, std):\n",
      "0.687292 0.300118\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 66.28\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 90.86\n",
      "AUROC (%): 82.14\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 90.77\n",
      "AUROC (%): 81.7\n",
      "Normality base rate (%): 64.41\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 92.07\n",
      "AUROC (%): 85.91\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 92.09\n",
      "AUROC (%): 85.79\n",
      "\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 33.72\n",
      "KL[p||u]: Abnormality Detection\n",
      "AUPR (%): 62.47\n",
      "AUROC (%): 82.14\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 61.98\n",
      "AUROC (%): 81.7\n",
      "Abnormality base rate (%): 35.59\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 73.02\n",
      "AUROC (%): 85.91\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 73.61\n",
      "AUROC (%): 85.79\n"
     ]
    }
   ],
   "source": [
    "print('Held-out subjects\\n')\n",
    "show_ood_detection_results(err, test_in_sample_examples, test_oos_examples)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
