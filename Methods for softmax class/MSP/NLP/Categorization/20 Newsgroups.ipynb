{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import sklearn.metrics as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename='./data/20ng-train.txt'):\n",
    "    '''\n",
    "    :param filename: the system location of the data to load\n",
    "    :return: the text (x) and its label (y)\n",
    "             the text is a list of words and is not processed\n",
    "    '''\n",
    "\n",
    "    # stop words taken from nltk\n",
    "    stop_words = ['i','me','my','myself','we','our','ours','ourselves','you','your','yours',\n",
    "                  'yourself','yourselves','he','him','his','himself','she','her','hers','herself',\n",
    "                  'it','its','itself','they','them','their','theirs','themselves','what','which',\n",
    "                  'who','whom','this','that','these','those','am','is','are','was','were','be',\n",
    "                  'been','being','have','has','had','having','do','does','did','doing','a','an',\n",
    "                  'the','and','but','if','or','because','as','until','while','of','at','by','for',\n",
    "                  'with','about','against','between','into','through','during','before','after',\n",
    "                  'above','below','to','from','up','down','in','out','on','off','over','under',\n",
    "                  'again','further','then','once','here','there','when','where','why','how','all',\n",
    "                  'any','both','each','few','more','most','other','some','such','no','nor','not',\n",
    "                  'only','own','same','so','than','too','very','s','t','can','will','just','don',\n",
    "                  'should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn',\n",
    "                  'doesn','hadn','hasn','haven','isn','ma','mightn','mustn','needn','shan',\n",
    "                  'shouldn','wasn','weren','won','wouldn']\n",
    "\n",
    "    x, y = [], []\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            line = re.sub(r'\\W+', ' ', line).strip()\n",
    "            x.append(line[1:])\n",
    "            x[-1] = ' '.join(word for word in x[-1].split() if word not in stop_words)\n",
    "            y.append(line[0])\n",
    "    return x, np.array(y, dtype=int)\n",
    "\n",
    "def get_vocab(dataset):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "\n",
    "    :return: a _ordered_ dictionary from words to counts\n",
    "    '''\n",
    "    vocab = {}\n",
    "\n",
    "    # create a counter for each word\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] += 1\n",
    "    \n",
    "    # sort from greatest to least by count\n",
    "    return collections.OrderedDict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "def text_to_rank(dataset, _vocab, desired_vocab_size=15000):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "    :vocab: a _ordered_ dictionary of vocab words and counts from get_vocab\n",
    "    :param desired_vocab_size: the desired vocabulary size\n",
    "    words no longer in vocab become UUUNNNKKK\n",
    "    :return: the text corpus with words mapped to their vocab rank,\n",
    "    with all sufficiently infrequent words mapped to UUUNNNKKK; UUUNNNKKK has rank desired_vocab_size\n",
    "    (the infrequent word cutoff is determined by desired_vocab size)\n",
    "    '''\n",
    "    _dataset = dataset[:]     # aliasing safeguard\n",
    "    vocab_ordered = list(_vocab)\n",
    "    count_cutoff = _vocab[vocab_ordered[desired_vocab_size-1]] # get word by its rank and map to its count\n",
    "    \n",
    "    word_to_rank = {}\n",
    "    for i in range(len(vocab_ordered)):\n",
    "        # we add one to make room for any future padding symbol with value 0\n",
    "        word_to_rank[vocab_ordered[i]] = i + 1\n",
    "    \n",
    "    for i in range(len(_dataset)):\n",
    "        example = _dataset[i]\n",
    "        example_as_list = example.split()\n",
    "        for j in range(len(example_as_list)):\n",
    "            try:\n",
    "                if _vocab[example_as_list[j]] >= count_cutoff and word_to_rank[example_as_list[j]] < desired_vocab_size:\n",
    "                    # we need to ensure that other words below the word on the edge of our desired_vocab size\n",
    "                    # are not also on the count cutoff\n",
    "                    example_as_list[j] = word_to_rank[example_as_list[j]] \n",
    "                else:\n",
    "                    example_as_list[j] = desired_vocab_size  # UUUNNNKKK\n",
    "            except:\n",
    "                example_as_list[j] = desired_vocab_size  # UUUNNNKKK\n",
    "        _dataset[i] = example_as_list\n",
    "\n",
    "    return _dataset\n",
    "\n",
    "def get_vocab(dataset):\n",
    "    '''\n",
    "    :param dataset: the text from load_data\n",
    "\n",
    "    :return: a _ordered_ dictionary from words to counts\n",
    "    '''\n",
    "    vocab = {}\n",
    "\n",
    "    # create a counter for each word\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] = 0\n",
    "\n",
    "    for example in dataset:\n",
    "        example_as_list = example.split()\n",
    "        for word in example_as_list:\n",
    "            vocab[word] += 1\n",
    "\n",
    "    # sort from greatest to least by count\n",
    "    return collections.OrderedDict(sorted(vocab.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# taken from keras\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32',\n",
    "                  padding='pre', truncating='pre', value=0.):\n",
    "    '''Pads each sequence to the same length:\n",
    "    the length of the longest sequence.\n",
    "    If maxlen is provided, any sequence longer\n",
    "    than maxlen is truncated to maxlen.\n",
    "    Truncation happens off either the beginning (default) or\n",
    "    the end of the sequence.\n",
    "    Supports post-padding and pre-padding (default).\n",
    "    # Arguments\n",
    "        sequences: list of lists where each element is a sequence\n",
    "        maxlen: int, maximum length\n",
    "        dtype: type to cast the resulting sequence.\n",
    "        padding: 'pre' or 'post', pad either before or after each sequence.\n",
    "        truncating: 'pre' or 'post', remove values from sequences larger than\n",
    "            maxlen either in the beginning or in the end of the sequence\n",
    "        value: float, value to pad the sequences to the desired value.\n",
    "    # Returns\n",
    "        x: numpy array with dimensions (number_of_sequences, maxlen)\n",
    "    '''\n",
    "    lengths = [len(s) for s in sequences]\n",
    "\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def partion_data_in_two(dataset, dataset_labels, in_sample_labels, oos_labels):\n",
    "    '''\n",
    "    :param dataset: the text from text_to_rank\n",
    "    :param dataset_labels: dataset labels\n",
    "    :param in_sample_labels: a list of newsgroups which the network will/did train on\n",
    "    :param oos_labels: the complement of in_sample_labels; these newsgroups the network has never seen\n",
    "    :return: the dataset partitioned into in_sample_examples, in_sample_labels,\n",
    "    oos_examples, and oos_labels in that order\n",
    "    '''\n",
    "    _dataset = dataset[:]     # aliasing safeguard\n",
    "    _dataset_labels = dataset_labels\n",
    "    \n",
    "    in_sample_idxs = np.zeros(np.shape(_dataset_labels), dtype=bool)\n",
    "    ones_vec = np.ones(np.shape(_dataset_labels), dtype=int)\n",
    "    for label in in_sample_labels:\n",
    "        in_sample_idxs = np.logical_or(in_sample_idxs, _dataset_labels == label * ones_vec)\n",
    "\n",
    "    \n",
    "    return _dataset[in_sample_idxs], _dataset_labels[in_sample_idxs],\\\n",
    "        _dataset[np.logical_not(in_sample_idxs)], _dataset_labels[np.logical_not(in_sample_idxs)]\n",
    "    \n",
    "# our network trains only on a subset of classes, say 6, but class number 7 might still\n",
    "# be an in-sample label: we need to squish the labels to be in {0,...,5}\n",
    "def relabel_in_sample_labels(labels):\n",
    "    labels_as_list = labels.tolist()\n",
    "    \n",
    "    set_of_labels = []\n",
    "    for label in labels_as_list:\n",
    "        set_of_labels.append(label)\n",
    "    labels_ordered = sorted(list(set(set_of_labels)))\n",
    "    \n",
    "    relabeled = np.zeros(labels.shape, dtype=int)\n",
    "    for i in range(len(labels_as_list)):\n",
    "        relabeled[i] = labels_ordered.index(labels_as_list[i])\n",
    "    \n",
    "    return relabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_example_len = 1000\n",
    "batch_size = 32\n",
    "embedding_dims = 30   # TODO: change to 50 and see what happens\n",
    "vocab_size = 15000\n",
    "num_epochs = 20\n",
    "hidden_dim = 128\n",
    "nclasses_to_exclude = 5  # 0-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_classes = np.arange(20)\n",
    "np.random.shuffle(random_classes)\n",
    "to_include = list(random_classes[:20-nclasses_to_exclude])\n",
    "to_exclude = list(random_classes[20-nclasses_to_exclude:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data\n",
      "Data loaded\n"
     ]
    }
   ],
   "source": [
    "print('Loading Data')\n",
    "X_train, Y_train = load_data()\n",
    "X_test, Y_test = load_data('./data/20ng-test.txt')\n",
    "\n",
    "vocab = get_vocab(X_train)\n",
    "X_train = text_to_rank(X_train, vocab, vocab_size)\n",
    "X_train = pad_sequences(X_train, maxlen=max_example_len)\n",
    "X_test = text_to_rank(X_test, vocab, vocab_size)\n",
    "X_test = pad_sequences(X_test, maxlen=max_example_len)\n",
    "\n",
    "# shuffle\n",
    "indices = np.arange(X_train.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_train = X_train[indices]\n",
    "Y_train = Y_train[indices]\n",
    "\n",
    "indices = np.arange(X_test.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "X_test = X_test[indices]\n",
    "Y_test = Y_test[indices]\n",
    "\n",
    "# split into train/dev\n",
    "X_dev = X_train[-1500:]\n",
    "Y_dev = Y_train[-1500:]\n",
    "X_train = X_train[:-1500]\n",
    "Y_train = Y_train[:-1500]\n",
    "\n",
    "in_sample_examples, in_sample_labels, oos_examples, oos_labels =\\\n",
    "partion_data_in_two(X_train, Y_train, to_include, to_exclude)\n",
    "dev_in_sample_examples, dev_in_sample_labels, dev_oos_examples, dev_oos_labels =\\\n",
    "partion_data_in_two(X_dev, Y_dev, to_include, to_exclude)\n",
    "test_in_sample_examples, test_in_sample_labels, test_oos_examples, dev_oos_labels =\\\n",
    "partion_data_in_two(X_test, Y_test, to_include, to_exclude)\n",
    "\n",
    "# safely assumes there is an example for each in_sample class in both the training and dev class \n",
    "in_sample_labels = relabel_in_sample_labels(in_sample_labels)\n",
    "dev_in_sample_labels = relabel_in_sample_labels(dev_in_sample_labels)\n",
    "test_in_sample_labels = relabel_in_sample_labels(test_in_sample_labels)\n",
    "\n",
    "num_examples = in_sample_labels.shape[0]\n",
    "num_batches = num_examples//batch_size\n",
    "\n",
    "print('Data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(dtype=tf.int32, shape=[None, max_example_len])\n",
    "    y = tf.placeholder(dtype=tf.int64, shape=[None])\n",
    "#     is_training = tf.placeholder(tf.bool)\n",
    "    \n",
    "    # add one to vocab size for the padding symbol\n",
    "    W_embedding = tf.Variable(\n",
    "        tf.random_uniform([vocab_size+1, embedding_dims])/tf.sqrt((vocab_size+1+embedding_dims)/6.),\n",
    "        trainable=True)\n",
    "    \n",
    "    w_vecs = tf.nn.embedding_lookup(W_embedding, x)\n",
    "    pooled = tf.reduce_mean(w_vecs, reduction_indices=[1])\n",
    "    \n",
    "    W_out = tf.Variable(tf.nn.l2_normalize(tf.random_normal([embedding_dims, 20-nclasses_to_exclude]), 0))\n",
    "    b_out = tf.Variable(tf.zeros([20-nclasses_to_exclude]))\n",
    "    \n",
    "    logits = tf.matmul(pooled, W_out) + b_out\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    lr = tf.train.exponential_decay(1e-2, global_step, 15*num_batches, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss, global_step=global_step)\n",
    "\n",
    "    acc = 100*tf.reduce_mean(tf.to_float(tf.equal(tf.argmax(logits, 1), y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "tf.initialize_all_variables().run()\n",
    "# create saver to train model\n",
    "saver = tf.train.Saver(max_to_keep=1)\n",
    "\n",
    "print('Initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Minibatch loss 1.465 | Minibatch accuracy 56.250 | Dev accuracy 62.717\n",
      "Epoch 2 | Minibatch loss 1.242 | Minibatch accuracy 68.750 | Dev accuracy 62.796\n",
      "Epoch 3 | Minibatch loss 0.663 | Minibatch accuracy 75.000 | Dev accuracy 63.823\n",
      "Epoch 4 | Minibatch loss 0.866 | Minibatch accuracy 84.375 | Dev accuracy 65.877\n",
      "Epoch 5 | Minibatch loss 0.746 | Minibatch accuracy 78.125 | Dev accuracy 73.065\n",
      "Epoch 6 | Minibatch loss 0.647 | Minibatch accuracy 78.125 | Dev accuracy 77.962\n",
      "Epoch 7 | Minibatch loss 0.327 | Minibatch accuracy 93.750 | Dev accuracy 82.385\n",
      "Epoch 8 | Minibatch loss 0.238 | Minibatch accuracy 87.500 | Dev accuracy 72.828\n",
      "Epoch 9 | Minibatch loss 0.459 | Minibatch accuracy 84.375 | Dev accuracy 80.332\n",
      "Epoch 10 | Minibatch loss 0.167 | Minibatch accuracy 96.875 | Dev accuracy 84.202\n",
      "Epoch 11 | Minibatch loss 0.047 | Minibatch accuracy 100.000 | Dev accuracy 86.414\n",
      "Epoch 12 | Minibatch loss 0.249 | Minibatch accuracy 100.000 | Dev accuracy 86.098\n",
      "Epoch 13 | Minibatch loss 0.191 | Minibatch accuracy 93.750 | Dev accuracy 91.074\n",
      "Epoch 14 | Minibatch loss 0.408 | Minibatch accuracy 78.125 | Dev accuracy 89.573\n",
      "Epoch 15 | Minibatch loss 0.246 | Minibatch accuracy 90.625 | Dev accuracy 89.731\n",
      "Epoch 16 | Minibatch loss 0.087 | Minibatch accuracy 100.000 | Dev accuracy 93.523\n",
      "Epoch 17 | Minibatch loss 0.159 | Minibatch accuracy 93.750 | Dev accuracy 95.024\n",
      "Epoch 18 | Minibatch loss 0.226 | Minibatch accuracy 96.875 | Dev accuracy 93.207\n",
      "Epoch 19 | Minibatch loss 0.107 | Minibatch accuracy 100.000 | Dev accuracy 92.338\n",
      "Epoch 20 | Minibatch loss 0.115 | Minibatch accuracy 96.875 | Dev accuracy 94.392\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # shuffle data every epoch\n",
    "    indices = np.arange(num_examples)\n",
    "    np.random.shuffle(indices)\n",
    "    in_sample_examples = in_sample_examples[indices]\n",
    "    in_sample_labels = in_sample_labels[indices]\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        offset = i * batch_size\n",
    "\n",
    "        x_batch = in_sample_examples[offset:offset + batch_size]\n",
    "        y_batch = in_sample_labels[offset:offset + batch_size]\n",
    "\n",
    "        _, l, batch_acc = sess.run([optimizer, loss, acc], feed_dict={x: x_batch, y: y_batch})\n",
    "\n",
    "    curr_dev_acc = sess.run(\n",
    "        acc, feed_dict={x: dev_in_sample_examples, y: dev_in_sample_labels})\n",
    "    if best_acc < curr_dev_acc:\n",
    "        best_acc = curr_dev_acc\n",
    "        saver.save(sess, './data/best_newsgroup_model.ckpt')\n",
    "\n",
    "    print('Epoch %d | Minibatch loss %.3f | Minibatch accuracy %.3f | Dev accuracy %.3f' %\n",
    "          (epoch+1, l, batch_acc, curr_dev_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model restored!\n",
      "Dev accuracy: 95.0237\n"
     ]
    }
   ],
   "source": [
    "# restore variables from disk\n",
    "saver.restore(sess, \"./data/best_newsgroup_model.ckpt\")\n",
    "print(\"Best model restored!\")\n",
    "\n",
    "print('Dev accuracy:', sess.run(acc, feed_dict={x: dev_in_sample_examples, y: dev_in_sample_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tf.nn.softmax(logits)\n",
    "s_prob = tf.reduce_max(s, reduction_indices=[1], keep_dims=True)\n",
    "kl_all = tf.log(20. - nclasses_to_exclude)\\\n",
    "        + tf.reduce_sum(s * tf.log(tf.abs(s) + 1e-10), reduction_indices=[1], keep_dims=True)\n",
    "m_all, v_all = tf.nn.moments(kl_all, axes=[0])\n",
    "\n",
    "logits_right = tf.boolean_mask(logits, tf.equal(tf.argmax(logits, 1), y))\n",
    "s_right = tf.nn.softmax(logits_right)\n",
    "s_right_prob = tf.reduce_max(s_right, reduction_indices=[1], keep_dims=True)\n",
    "kl_right = tf.log(20. - nclasses_to_exclude)\\\n",
    "         + tf.reduce_sum(s_right * tf.log(tf.abs(s_right) + 1e-10), reduction_indices=[1], keep_dims=True)\n",
    "m_right, v_right = tf.nn.moments(kl_right, axes=[0])\n",
    "\n",
    "logits_wrong = tf.boolean_mask(logits, tf.not_equal(tf.argmax(logits, 1), y))\n",
    "s_wrong = tf.nn.softmax(logits_wrong)\n",
    "s_wrong_prob = tf.reduce_max(s_wrong, reduction_indices=[1], keep_dims=True)\n",
    "kl_wrong = tf.log(20. - nclasses_to_exclude)\\\n",
    "           + tf.reduce_sum(s_wrong * tf.log(tf.abs(s_wrong) + 1e-10), reduction_indices=[1], keep_dims=True)\n",
    "m_wrong, v_wrong = tf.nn.moments(kl_wrong, axes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 NG (w/class subset) Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):\n",
      "7.3129 | 0.861871 0.205038 | 0.888052 0.178718 | 0.530052 0.226419\n",
      "\n",
      "Success Detection\n",
      "Success base rate (%): 92.69\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 98.79\n",
      "AUROC (%): 87.43\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 98.9\n",
      "AUROC (%): 88.68\n",
      "\n",
      "Error Detection\n",
      "Error base rate (%): 7.31\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 37.91\n",
      "AUROC (%): 87.43\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 41.66\n",
      "AUROC (%): 88.68\n"
     ]
    }
   ],
   "source": [
    "err, kl_a, kl_r, kl_w, s_p, s_rp, s_wp = sess.run(\n",
    "    [100 - acc, kl_all, kl_right, kl_wrong, s_prob, s_right_prob, s_wrong_prob],\n",
    "    feed_dict={x: test_in_sample_examples, y: test_in_sample_labels})\n",
    "\n",
    "print('20 NG (w/class subset) Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):')\n",
    "print(err, '|', np.mean(s_p), np.std(s_p), '|', np.mean(s_rp), np.std(s_rp), '|', np.mean(s_wp), np.std(s_wp))\n",
    "\n",
    "print('\\nSuccess Detection')\n",
    "print('Success base rate (%):', round(100-err,2))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "safe, risky = kl_r, kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Prediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = s_rp, s_wp\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "\n",
    "print('\\nError Detection')\n",
    "print('Error base rate (%):', round(err,2))\n",
    "safe, risky = -kl_r, -kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Prediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = -s_rp, -s_wp\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_ood_detection_results(error_rate_for_in, in_examples, out_examples):\n",
    "    kl_oos, s_p_oos = sess.run([kl_all, s_prob], feed_dict={x: out_examples})\n",
    "\n",
    "    print('OOD Example Prediction Probability (mean, std):')\n",
    "    print(np.mean(s_p_oos), np.std(s_p_oos))\n",
    "\n",
    "    print('\\nNormality Detection')\n",
    "    print('Normality base rate (%):', round(100*in_examples.shape[0]/(\n",
    "                out_examples.shape[0] + in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Normality Detection')\n",
    "    safe, risky = kl_a, kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection')\n",
    "    safe, risky = s_p, s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Normality base rate (%):', round(100*(1 - err/100)*in_examples.shape[0]/\n",
    "          (out_examples.shape[0] + (1 - err/100)*in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = kl_r, kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Normality Detection (relative to correct examples)')\n",
    "    safe, risky = s_rp, s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[:safe.shape[0]] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "\n",
    "    print('\\n\\nAbnormality Detection')\n",
    "    print('Abnormality base rate (%):', round(100*out_examples.shape[0]/(\n",
    "                out_examples.shape[0] + in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Abnormality Detection')\n",
    "    safe, risky = -kl_a, -kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Abnormality Detection')\n",
    "    safe, risky = -s_p, -s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Abnormality base rate (%):', round(100*out_examples.shape[0]/\n",
    "          (out_examples.shape[0] + (1 - err/100)*in_examples.shape[0]),2))\n",
    "    print('KL[p||u]: Abnormality Detection (relative to correct examples)')\n",
    "    safe, risky = -kl_r, -kl_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "    print('Prediction Prob: Abnormality Detection (relative to correct examples)')\n",
    "    safe, risky = -s_rp, -s_p_oos\n",
    "    labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "    labels[safe.shape[0]:] += 1\n",
    "    examples = np.squeeze(np.vstack((safe, risky)))\n",
    "    print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "    print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Held-out subjects\n",
      "\n",
      "OOD Example Prediction Probability (mean, std):\n",
      "0.645483 0.272967\n",
      "\n",
      "Normality Detection\n",
      "Normality base rate (%): 85.37\n",
      "KL[p||u]: Normality Detection\n",
      "AUPR (%): 92.21\n",
      "AUROC (%): 72.1\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 92.27\n",
      "AUROC (%): 72.24\n",
      "Normality base rate (%): 84.4\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 92.31\n",
      "AUROC (%): 74.68\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 92.4\n",
      "AUROC (%): 74.97\n",
      "\n",
      "\n",
      "Abnormality Detection\n",
      "Abnormality base rate (%): 14.63\n",
      "KL[p||u]: Abnormality Detection\n",
      "AUPR (%): 33.48\n",
      "AUROC (%): 72.1\n",
      "Prediction Prob: Normality Detection\n",
      "AUPR (%): 33.99\n",
      "AUROC (%): 72.24\n",
      "Abnormality base rate (%): 15.6\n",
      "KL[p||u]: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 43.28\n",
      "AUROC (%): 74.68\n",
      "Prediction Prob: Normality Detection (relative to correct examples)\n",
      "AUPR (%): 44.74\n",
      "AUROC (%): 74.97\n"
     ]
    }
   ],
   "source": [
    "print('Held-out subjects\\n')\n",
    "show_ood_detection_results(err, test_in_sample_examples, test_oos_examples)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
