{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# %matplotlib inline\n",
    "# import matplotlib.pylab as plt\n",
    "from helper_functions_twitter import *\n",
    "import sklearn.metrics as sk\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window_size = 1\n",
    "\n",
    "# note that we encode the tags with numbers for later convenience\n",
    "tag_to_number = {\n",
    "    u'N': 0, u'O': 1, u'S': 2, u'^': 3, u'Z': 4, u'L': 5, u'M': 6,\n",
    "    u'V': 7, u'A': 8, u'R': 9, u'!': 10, u'D': 11, u'P': 12, u'&': 13, u'T': 14,\n",
    "    u'X': 15, u'Y': 16, u'#': 17, u'@': 18, u'~': 19, u'U': 20, u'E': 21, u'$': 22,\n",
    "    u',': 23, u'G': 24\n",
    "}\n",
    "\n",
    "embeddings = embeddings_to_dict('./data/Tweets/embeddings-twitter.txt')\n",
    "vocab = embeddings.keys()\n",
    "\n",
    "# we replace <s> with </s> since it has no embedding, and </s> is a better embedding than UNK\n",
    "xt, yt = data_to_mat('./data/Tweets/tweets-train.txt', vocab, tag_to_number, window_size=window_size,\n",
    "                     start_symbol=u'</s>')\n",
    "xdev, ydev = data_to_mat('./data/Tweets/tweets-dev.txt', vocab, tag_to_number, window_size=window_size,\n",
    "                         start_symbol=u'</s>')\n",
    "xdtest, ydtest = data_to_mat('./data/Tweets/tweets-devtest.txt', vocab, tag_to_number, window_size=window_size,\n",
    "                             start_symbol=u'</s>')\n",
    "\n",
    "data = {\n",
    "    'x_train': xt, 'y_train': yt,\n",
    "    'x_dev': xdev, 'y_dev': ydev,\n",
    "    'x_devtest': xdtest, 'y_devtest': ydtest\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build tf inputs\n",
    "num_epochs = 30\n",
    "num_tags = 25\n",
    "hidden_size = 256\n",
    "batch_size = 64\n",
    "embedding_dimension = 50\n",
    "example_size = (2*window_size + 1)*embedding_dimension\n",
    "init_lr = 0.001\n",
    "num_examples = data['y_train'].shape[0]\n",
    "num_batches = num_examples//batch_size\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    x = tf.placeholder(tf.float32, [None, example_size])\n",
    "    y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "    w1 = tf.Variable(tf.nn.l2_normalize(tf.random_normal([example_size, hidden_size]), 0)/tf.sqrt(1 + 0.425))\n",
    "    b1 = tf.Variable(tf.zeros([hidden_size]))\n",
    "    w2 = tf.Variable(tf.nn.l2_normalize(tf.random_normal([hidden_size, hidden_size]), 0)/tf.sqrt(0.425 + 0.425))\n",
    "    b2 = tf.Variable(tf.zeros([hidden_size]))\n",
    "    w_out = tf.Variable(tf.nn.l2_normalize(tf.random_normal([hidden_size, num_tags]), 0)/tf.sqrt(0.425 + 1))\n",
    "    b_out = tf.Variable(tf.zeros([num_tags]))\n",
    "\n",
    "    def gelu_fast(_x):\n",
    "        return 0.5 * _x * (1 + tf.tanh(tf.sqrt(2 / np.pi) * (_x + 0.044715 * tf.pow(_x, 3))))\n",
    "\n",
    "    def model(data_feed):\n",
    "        h1 = gelu_fast(tf.matmul(data_feed, w1) + b1)\n",
    "        h2 = gelu_fast(tf.matmul(h1, w2) + b2)\n",
    "        return tf.matmul(h2, w_out) + b_out\n",
    "\n",
    "    logits = model(x)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, y))\n",
    "    loss += 5e-5*(tf.nn.l2_loss(w1) + tf.nn.l2_loss(w2))\n",
    "\n",
    "    # learning rate annealing\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    # drop lr 15 epochs in\n",
    "    lr = tf.train.exponential_decay(init_lr, global_step, 15*num_batches, 0.1, staircase=True)\n",
    "    # pick optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)\n",
    "\n",
    "    acc = 100*tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), y), \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "tf.initialize_all_variables().run()\n",
    "print('Initialized')\n",
    "\n",
    "# create saver to train model\n",
    "saver = tf.train.Saver(max_to_keep=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Minibatch loss 0.644 | Minibatch accuracy 82.812 | Dev accuracy 81.775\n",
      "Epoch 1 | Minibatch loss 0.572 | Minibatch accuracy 85.938 | Dev accuracy 84.325\n",
      "Epoch 2 | Minibatch loss 0.601 | Minibatch accuracy 89.062 | Dev accuracy 85.383\n",
      "Epoch 3 | Minibatch loss 0.639 | Minibatch accuracy 78.125 | Dev accuracy 85.403\n",
      "Epoch 4 | Minibatch loss 0.249 | Minibatch accuracy 92.188 | Dev accuracy 86.419\n",
      "Epoch 5 | Minibatch loss 0.318 | Minibatch accuracy 90.625 | Dev accuracy 86.502\n",
      "Epoch 6 | Minibatch loss 0.231 | Minibatch accuracy 95.312 | Dev accuracy 86.668\n",
      "Epoch 7 | Minibatch loss 0.495 | Minibatch accuracy 82.812 | Dev accuracy 86.958\n",
      "Epoch 8 | Minibatch loss 0.485 | Minibatch accuracy 87.500 | Dev accuracy 87.186\n",
      "Epoch 9 | Minibatch loss 0.399 | Minibatch accuracy 90.625 | Dev accuracy 86.917\n",
      "Epoch 10 | Minibatch loss 0.403 | Minibatch accuracy 90.625 | Dev accuracy 87.290\n",
      "Epoch 11 | Minibatch loss 0.206 | Minibatch accuracy 92.188 | Dev accuracy 86.896\n",
      "Epoch 12 | Minibatch loss 0.244 | Minibatch accuracy 93.750 | Dev accuracy 86.958\n",
      "Epoch 13 | Minibatch loss 0.239 | Minibatch accuracy 95.312 | Dev accuracy 86.585\n",
      "Epoch 14 | Minibatch loss 0.155 | Minibatch accuracy 95.312 | Dev accuracy 86.668\n",
      "Epoch 15 | Minibatch loss 0.141 | Minibatch accuracy 95.312 | Dev accuracy 87.207\n",
      "Epoch 16 | Minibatch loss 0.366 | Minibatch accuracy 90.625 | Dev accuracy 87.311\n",
      "Epoch 17 | Minibatch loss 0.180 | Minibatch accuracy 93.750 | Dev accuracy 87.414\n",
      "Epoch 18 | Minibatch loss 0.186 | Minibatch accuracy 95.312 | Dev accuracy 87.269\n",
      "Epoch 19 | Minibatch loss 0.132 | Minibatch accuracy 96.875 | Dev accuracy 87.103\n",
      "Epoch 20 | Minibatch loss 0.092 | Minibatch accuracy 100.000 | Dev accuracy 87.124\n",
      "Epoch 21 | Minibatch loss 0.158 | Minibatch accuracy 96.875 | Dev accuracy 87.021\n",
      "Epoch 22 | Minibatch loss 0.073 | Minibatch accuracy 100.000 | Dev accuracy 87.166\n",
      "Epoch 23 | Minibatch loss 0.104 | Minibatch accuracy 96.875 | Dev accuracy 87.103\n",
      "Epoch 24 | Minibatch loss 0.146 | Minibatch accuracy 95.312 | Dev accuracy 87.103\n",
      "Epoch 25 | Minibatch loss 0.244 | Minibatch accuracy 95.312 | Dev accuracy 87.062\n",
      "Epoch 26 | Minibatch loss 0.227 | Minibatch accuracy 92.188 | Dev accuracy 87.145\n",
      "Epoch 27 | Minibatch loss 0.169 | Minibatch accuracy 96.875 | Dev accuracy 86.772\n",
      "Epoch 28 | Minibatch loss 0.147 | Minibatch accuracy 95.312 | Dev accuracy 86.917\n",
      "Epoch 29 | Minibatch loss 0.098 | Minibatch accuracy 96.875 | Dev accuracy 86.896\n"
     ]
    }
   ],
   "source": [
    "best_acc = 0\n",
    "\n",
    "# train\n",
    "for epoch in range(num_epochs):\n",
    "    # shuffle data every epoch\n",
    "    indices = np.arange(num_examples)\n",
    "    np.random.shuffle(indices)\n",
    "    data['x_train'] = data['x_train'][indices]\n",
    "    data['y_train'] = data['y_train'][indices]\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        offset = i * batch_size\n",
    "\n",
    "        x_batch = word_list_to_embedding(data['x_train'][offset:offset + batch_size, :],\n",
    "                                             embeddings, embedding_dimension)\n",
    "        y_batch = data['y_train'][offset:offset + batch_size]\n",
    "\n",
    "        _, l, batch_acc = sess.run([optimizer, loss, acc],\n",
    "                                     feed_dict={x: x_batch, y: y_batch})\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            curr_dev_acc = sess.run(\n",
    "                acc, feed_dict={x: word_list_to_embedding(data['x_dev'], embeddings, embedding_dimension),\n",
    "                                y: data['y_dev']})\n",
    "            if best_acc < curr_dev_acc:\n",
    "                best_acc = curr_dev_acc\n",
    "                saver.save(sess, './data/best_tweet_model.ckpt')\n",
    "\n",
    "    print('Epoch %d | Minibatch loss %.3f | Minibatch accuracy %.3f | Dev accuracy %.3f' %\n",
    "          (epoch, l, batch_acc, curr_dev_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model restored!\n",
      "DevTest accuracy: 87.3462\n"
     ]
    }
   ],
   "source": [
    "# restore variables from disk\n",
    "saver.restore(sess, \"./data/best_tweet_model.ckpt\")\n",
    "print(\"Best model restored!\")\n",
    "\n",
    "print('DevTest accuracy:', sess.run(\n",
    "        acc, feed_dict={x: word_list_to_embedding(data['x_devtest'], embeddings, embedding_dimension),\n",
    "                        y: data['y_devtest']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = tf.nn.softmax(logits)\n",
    "s_prob = tf.reduce_max(s, reduction_indices=[1], keep_dims=True)\n",
    "kl_all = tf.log(25.) + tf.reduce_sum(s * tf.log(tf.abs(s) + 1e-11), reduction_indices=[1], keep_dims=True)\n",
    "m_all, v_all = tf.nn.moments(kl_all, axes=[0])\n",
    "\n",
    "logits_right = tf.boolean_mask(logits, tf.equal(tf.argmax(logits, 1), y))\n",
    "s_right = tf.nn.softmax(logits_right)\n",
    "s_right_prob = tf.reduce_max(s_right, reduction_indices=[1], keep_dims=True)\n",
    "kl_right = tf.log(25.) + tf.reduce_sum(s_right * tf.log(tf.abs(s_right) + 1e-11), reduction_indices=[1], keep_dims=True)\n",
    "m_right, v_right = tf.nn.moments(kl_right, axes=[0])\n",
    "\n",
    "logits_wrong = tf.boolean_mask(logits, tf.not_equal(tf.argmax(logits, 1), y))\n",
    "s_wrong = tf.nn.softmax(logits_wrong)\n",
    "s_wrong_prob = tf.reduce_max(s_wrong, reduction_indices=[1], keep_dims=True)\n",
    "kl_wrong = tf.log(25.) + tf.reduce_sum(s_wrong * tf.log(tf.abs(s_wrong) + 1e-11), reduction_indices=[1], keep_dims=True)\n",
    "m_wrong, v_wrong = tf.nn.moments(kl_wrong, axes=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):\n",
      "12.5855 | 0.91604 0.162581 | 0.949234 0.119917 | 0.685485 0.222114\n",
      "\n",
      "Success Detection\n",
      "Success base rate (%): 87.41\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 98.23\n",
      "AUROC (%): 89.23\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 98.24\n",
      "AUROC (%): 89.28\n",
      "\n",
      "Error Detection\n",
      "Error base rate (%): 12.59\n",
      "KL[p||u]: Right/Wrong classification distinction\n",
      "AUPR (%): 53.51\n",
      "AUROC (%): 89.23\n",
      "Prediction Prob: Right/Wrong classification distinction\n",
      "AUPR (%): 53.27\n",
      "AUROC (%): 89.28\n"
     ]
    }
   ],
   "source": [
    "err, kl_a, kl_r, kl_w, s_p, s_rp, s_wp = sess.run(\n",
    "    [100 - acc, kl_all, kl_right, kl_wrong, s_prob, s_right_prob, s_wrong_prob],\n",
    "    feed_dict={x: word_list_to_embedding(data['x_dev'],embeddings, embedding_dimension),\n",
    "               y: data['y_dev']})\n",
    "\n",
    "print('Twitter Error (%)| Prediction Prob (mean, std) | PProb Right (mean, std) | PProb Wrong (mean, std):')\n",
    "print(err, '|', np.mean(s_p), np.std(s_p), '|', np.mean(s_rp), np.std(s_rp), '|', np.mean(s_wp), np.std(s_wp))\n",
    "\n",
    "print('\\nSuccess Detection')\n",
    "print('Success base rate (%):', round(100-err,2))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "safe, risky = kl_r, kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Prediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = s_rp, s_wp\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[:safe.shape[0]] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "\n",
    "print('\\nError Detection')\n",
    "print('Error base rate (%):', round(err,2))\n",
    "safe, risky = -kl_r, -kl_w\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('KL[p||u]: Right/Wrong classification distinction')\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))\n",
    "\n",
    "print('Prediction Prob: Right/Wrong classification distinction')\n",
    "safe, risky = -s_rp, -s_wp\n",
    "labels = np.zeros((safe.shape[0] + risky.shape[0]), dtype=np.int32)\n",
    "labels[safe.shape[0]:] += 1\n",
    "examples = np.squeeze(np.vstack((safe, risky)))\n",
    "print('AUPR (%):', round(100*sk.average_precision_score(labels, examples), 2))\n",
    "print('AUROC (%):', round(100*sk.roc_auc_score(labels, examples), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
